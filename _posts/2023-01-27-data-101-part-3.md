---
layout: post
title: Data 101 - part 3
subtitle: Data journey
thumbnail-img: https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGVfhxhGXNWmcz3KjVzMDqz-tzg9eoB8cM5FOM0MB0gb8Qq9K4JMHl8klMWGUKlv00JDq-3QWMP5o30iXbZQs4UKzdFmlmk_0zI_nNthrmckHdqMybOayDxOg3QVYn847k6810U_ObpOoll_eXzMRbfEkKIcAh-A0v5qpSm0c0HJEYDqTSSe1qeLYG
share-img: https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGVfhxhGXNWmcz3KjVzMDqz-tzg9eoB8cM5FOM0MB0gb8Qq9K4JMHl8klMWGUKlv00JDq-3QWMP5o30iXbZQs4UKzdFmlmk_0zI_nNthrmckHdqMybOayDxOg3QVYn847k6810U_ObpOoll_eXzMRbfEkKIcAh-A0v5qpSm0c0HJEYDqTSSe1qeLYG
tags: [data journey,data warehouse,data lifecycle,data analysis,OLAP,data value chain,SQL,data mart,data storage,file,NoSQL,data lakehouse,data ingestion,block,OLTP,data lake,object]
comments: true
---

In the last episode, we've seen that successful data strategies require reviewing existing data platforms and analyzing how business users can take advantage of them. Any data strategy requires the right tools and technologies to work as planned. Here, data architecture takes a pivotal role to choose the right tools and processes that allow you to design the adequate data platform for your organization. 

In this article, I will introduce the concept of data journey and how raw data increase in value thorough its multiple stages.

## Overview

The Data journey or the data value chain describes the different steps in which data goes from its creation to its eventual disposal. In the context of data platforms, It consists of ingestion, storage, processing, and dissemination (sharing, visualization, monetization...). The goal of this process is to ensure that the right information is available at the right time for decision-making purposes while also protecting it against unauthorized access or misuse.

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGVfhxhGXNWmcz3KjVzMDqz-tzg9eoB8cM5FOM0MB0gb8Qq9K4JMHl8klMWGUKlv00JDq-3QWMP5o30iXbZQs4UKzdFmlmk_0zI_nNthrmckHdqMybOayDxOg3QVYn847k6810U_ObpOoll_eXzMRbfEkKIcAh-A0v5qpSm0c0HJEYDqTSSe1qeLYG){: .mx-auto.d-block :} *A modern data architecture represents all stages of the data journey ©Semantix*{:style="display:block; margin-left:auto; margin-right:auto; text-align: center"}  

To create a performant and adequate data journey for business users, the data architects focus on how technological tools enable your organization and consequently your people to be more data-driven (see [part 2](https://aelkouhen.github.io/2023-01-24-data-101-part-2/)). Far from the common mindset of modernizing to modernize, he relies on few strict rules :

*   _Relevance_: Who will be using the tech, and will it meet their needs? Technology should serve the business users and not the opposite. Also, ensure that each stage of data journey has the right technology and processes in place to maintain data integrity and produce the most value. Instead of identifying a universal best-in-class approach, use a customized tool selection based on the data characteristics (see [part 1](https://aelkouhen.github.io/2023-01-22-data-101-part-1/)), your team size and the maturity level for your organisation.
*   _Accessibility_: Data architects should consider tools and technologies that enable everyone across the organization to make data-driven decisions without any obstacles when accessing data.
*   _Performance_: There are powerful technologies on the market that speed the data transformation process. Consider tools that will enable business users to be proactive and not reactive.

## Data journey stages

The data journey consists of many stages. The main ones are: ingestion, storage, processing, analysis, and finally, dissemination. Each stage has its own set of activities and considerations.

Overall, understanding each stage of the data lifecycle will help you make better choices about tools and processes you apply to your data assets, depending on what you are expecting from the data platform.

### 1 - Data Ingestion

Data ingestion is the first stage of the data lifecycle. This is where data is collected from various internal sources like databases, CRM, ERPs, legacy systems and external ones such as surveys, and third-party providers. It’s important to ensure that the data being acquired is accurate and up\-to\-date so it can be used effectively in subsequent stages of the cycle.

In this stage, raw data are extracted from one or more data sources, replicated, then ingested into a landing storage support. To ensure data ingestion stage has the right technology and processes to meet its goals, you have to consider the characteristics of data you want to acquire. As I've explained in [part 1](https://aelkouhen.github.io/2023-01-22-data-101-part-1/), data has few innate characteristics. The most relevant ones for this stage: are Volume, Variety, and Velocity.

The ingestion layer can be decomposed in three sub-layers:

1. **_Data sources_**:  As a Data Architect you need to understand how the data is produced and what possible options to connect it with the Data System are. Also, Data Contracts will be ideally implemented here. The Data Sources that you will run into in almost every company can be  internal Services (REST or gRPC APIs emitting events), IoT (Internet of Things) devices,  website activity events pushed by the website code (tracking pixels) or scraped by a web scarping tool, third Party SaaS offerings (like CRMs or ERPs, Internal Backend Databases, or even some flat files (used mainly as reference data) when there is no Master Data Management System (MDM) available.

2. **_Data collection_**: Rarely the data produced by Data Sources will be pushed directly downstream. This subsystem acts as a proxy between External Sources and the Data System. You are very likely to see one of three proxy types here:

*   Collectors - applications that expose a public or private endpoint to which Data Producers can send the Events.
*   CDC Connectors - applications that connect to Backend DB event logs and push selected updates against the DB to the Data System.
*   Extractors - applications written by engineers that poll for changes in external systems like websites and push the Data down the stream.

3. **_Data transport_** : In most cases you will see a Distributed Messaging system before the Data moves down the stream. Also, data validation can happen at this stage to alleviate stress from computations happening down the Data value Chain. More often, we use the terms data transport and data ingestion interchangeably when we talk about the velocity of data acquisition.

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg98V1tem2vRj1Gc3uGaQJDyNo_lZI6BM_EYhb4tl529hbakML9WxFIPUs_CBIYcHgZRr6ktf40t7FDE5_jrcjWipro7HN81CRDTdypMxYDFAm9rmkPe_4nis46FppNcqM-cMRLZnww1gZDAdlSiGlxrbHFEtJSzUKy_imddfiJwpeOc64dGtLumqH4/w361-h453/ingestion-layer.png){: .mx-auto.d-block :} *Ingestion Layer @SWIRLAI*{:style="display:block; margin-left:auto; margin-right:auto; text-align: center"} 

While, most of acquisition tools can handle a high volume of data with a wide range of formats (structured, unstructured...), they differ in the way they handle the velocity of data. We often distinguish three main categories of data ingestion tools: batch-based, real-time or stream-based, and hybrid ones.

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjT-eVXe6tYthglvjfCcQkzm6cKW_QBIAocbWokf5A5c6yS3PNpT0cwDwcv2g1EfI672oj053EV3er1sLwZmPCcJoVK4F-sEfWYkkqb3xVwvUl6UiQ7pNgU_k9gVB-DZAm8Zy06F_lgqydmNxKogZ7mfU2Mf1JaMaA_5UbTQENLgoDKwbAcjzHKTlp2/w463-h459/Ingestion.png){: .mx-auto.d-block :} *Data acquisition*{:style="display:block; margin-left:auto; margin-right:auto; text-align: center"}  

Batch-based data ingestion is the process of collecting and transferring data in batches according to scheduled intervals. The ingestion layer may collect data based on simple schedules, trigger events, or any other logical ordering. Batch-based ingestion can be useful for companies that need to collect specific data points on a daily basis or don't require data for real-time decision making.

Real-time or Stream-based ingestion is essential for organizations to rapidly respond to new information in time-sensitive use cases, such as stock market trading or sensors monitoring. Real-time data acquisition is vital when making rapid operational decisions or acting on fresh insights.

Publish-subscribe (PubSub) messaging systems are relevant here, as they allow real-time applications to communicate their data by publishing messages. Receivers are subscribers of these systems and receive data as soon as it is published by the datasource. To handle the high amount of data, these kins of tools should be highly scalable and fault-tolerant.

Hybrid data ingestion is an incremental setup that consists of both batch and stream methods. It performs a delta function between the target and the source(s) and assess what is missing. Initially, there is no data in the target, thus, the hybrid ingestion tool extracts a snapshot of the datasources and transfer it as a batch to the landing zone. Once the initial load is done, it ingests further data as a stream. 

One of the most used tools in this category, is the Change Data Capture (CDC). The CDC constantly monitors the database transaction logs or redo logs and moves changed data as a stream without interfering with the database workload.

Data ingestion tools and ETL (Extract-Transform-Load) platforms may appear similar in function, but there are some differences. Data ingestion is primarily concerned with extracting data from the source and loading it into the target site. On the other hand, ETL is more complete since it involves not only the extraction and transfer of data, but also the transformation of that data before its delivery to target destinations.

### 2 - Data Storage

Storage refers to how data is stored once it has been acquired. Data should be securely stored on a reliable platform with appropriate backup systems in place for disaster recovery purposes if needed. Additionally, access control measures must also be implemented to protect sensitive information from unauthorized users or malicious actors who may try to gain access illegally.

Data storage has few characteristics such as storage lifecycle (how data will evolve), storage options (how data can be stored efficiently), storage layers, storage formats (how data should be stored), and the storage technologies in which data are kept.

### Storage lifecycle

The storage lifecycle is the process of managing stored data throughout its entire life cycle, from the initial creation in the support to eventual deletion. In your data platform, you should manage your data objects with a set of rules that define actions to apply when predefined conditions are verified. There are two types of actions:

*   **_Transition actions_**: These actions define when data objects transition from an isolated zone to another. In most data platforms, the landing zone is the first storage area in which raw data was ingested. It is necessary then to set transition actions to transfer data from a storage area to another one (eg., curated, augmented or conformed...). 
    
    You can define also when data objects transition from a storage class to another (eg., label data as cold when it is less frequently used, or archive data older than one year) which can give a better control on storage costs.
    
*   **_Expiration actions_**: you can define when your data should be removed (expired).

These kinds of actions are usually performed by ETL tools, but some storage services (especially those provided by Cloud Vendors) can offer such capabilities (eg., AWS S3 lifecycle policies).

### Storage layers

The data lakes architecture were initially thought to be the solution for storing massive amounts of data, but they have instead become data swamps filled with petabytes of structured and unstructured information that cannot be used.

To combat this issue, it is important to understand how to create zones within a data lake in order to effectively drain the swamp.

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4APG0odRWDdZquPXNJv3MW9IHFKGCkfWhJgFwB8CoaEZpTPEBMzKoF4oDEjyPFNiOMjjnixqW_eeRRnchpirpUkA--bWQsk0V8lscxIxfm9eId3XzYMUci7BqZ65IOerGXc4bx3_lYpOq7vFE_9MZ1ysN_Pia0ULSyVbHhOgat65jozPOTWZ62pIe){: .mx-auto.d-block :} *Storage layers: a cornerstone for modern data platforms*{:style="display:block; margin-left:auto; margin-right:auto; text-align: center"}   
    
Data lakes are divided into three main zones: Raw, Trusted, and Curated. Organizations may label these zones differently according to individual or industry preference, but their functions are essentially the same.
    
1.  **_Raw zone_**: also known as the landing zone, bronze zone or even the swamp. In this zone, data is stored in its native format, without transformation or binding to any business rules. Though all data starts in the raw zone, it’s too vast of a landscape for business end-users (less technical). Typical users of the raw zone include data engineers, data stewards, data analysts, and data scientists, who are defined by their ability to derive new knowledge and insights amid vast amounts of data. These users tend to spend a lot of time sifting through data, then pushing it into other zones.
2.  **_Trusted zone_**: also called the exploration zone, the sandbox, silver zone, or the pond. This is the place where data can be discovered and explored. It includes private zones for each individual user and a shared zone for team collaboration. The trusted data zone holds data that serves as universal "source of truth" for downstream zones and systems. A broader group of users has applied extensive governance to this data, which has more comprehensive definitions that the entire organization can stand behind.
3.  **_Curated zone_**: also known as refined data, production zone, gold zone, service zone or the lagoon. This is where clean, manipulated and augmented data are stored in the optimal format to drive business-decisions. It offers operational data stores that feed data warehouses and data marts. This zone has strict security measures in place to limit data access and provides automated delivery of read-only data for end users.

In some platforms, you can find additional zones called transient or transit zones that hold ephemeral unprocessed data (ie., temporary copies, streaming pools...), stored temporarily before being moved to one of the three permanent zones.

### Storage options

Storing data collected from multiple sources to a target storage system has always been a global challenge for organizations. The chosen storage option is particularly an area of interest for improving the data journey. You either employ the bulk method (also called full load) or the incremental Load method for performing data storage.

In a bulk storage, the entire target dataset is cleared out and then entirely overwritten : replaced with an updated version during each data loading run. Performing a full data load is an easy-to-implement process that requires minimal maintenance and has a simple design. This technique simply deletes the old dataset and replaces it with an entire updated dataset, eliminating the need to manage deltas (changes). Furthermore, if any errors occur during this loading process they can be easily rectified by rerunning it without having to do additional data cleanup/preparation.

However, using the full data load approach, one may experience difficulty in sustaining data loading with a small number of records (when you need to update only few values but you have to erase then insert millions of objects), slow performance when dealing with large datasets and an inability to preserve historical data.

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiR2FA0_qQ8acGmi_A6JAqCFf0uuTLt3NrglYlKmQX72bv_xoqm01dRJkDkYjcwSDynTfSesdWMJ3UWrYmdVmnJ6tLIYxabWkseMhv8KuZIG7JQcxOz0ZMjSyBgVroYas2Xt4hd39qFcPWWwhjSGjJxNI0_Pm__iorTuZOVI4Fwmtr2ztCptatyoxqM){: .mx-auto.d-block :} *Bulk vs Incremental storage*{:style="display:block; margin-left:auto; margin-right:auto; text-align: center"} 
  
On the other hand, incremental load is a selective method of storing data from one or many sources to a target storage location. The incremental load model will attempt to compare the incoming data from the source systems with the existing data present in the destination for any new or changed data values since the last loading operation. Some storage systems, keep track of every modification as a versioning control system could do.

The key advantage of incremental storage over full bulk load, is that it minimizes the amount of work that needs to be done with each batch. This can make incremental loading much faster overall, especially when dealing with large datasets. In addition, since only new or changed data is stored each time, there is less risk of errors introduced into the target datasets. Finally, storing history is often easier with an incremental load approach since all data from upstream sources can be retained in the target storage system as distinct versions or releases, without the need of duplicating the full datasets.

When using Incremental Data storage, there are potential challenges that can arise, like incompatibility issues that may occur when new records invalidate existing data due to incompatible formats or types of data being added. This can create a bottleneck as the end user cannot get consistent and reliable insights from these corrupted data. Additionally, distributed systems used for processing incoming data could result in changes or deletions occurring in a different sequence (out-of-order events) than when it was received originally.  

### Storage formats

Storage formats define the way that the storage layer holds, organizes, and presents data independently on the underlying storage support. data can be stored in different formats:

*   **_File storage_**: also known as file-level or file-based storage, is a method of storing data where each piece of information is kept inside a folder (hierarchical storage). To access the data, your computer needs to know the path it's located at. This type of systems (known as File System) uses limited metadata that tells the computer exactly where to find the files; similar to how you would use a library card catalog for books.
*   **_Block storage_** divides data into blocks and stores them individually, each with its own unique identifier. This allows the system to store these smaller pieces of information in whatever location is most efficient. Block storage is a great solution for businesses that require quick and reliable access to large amounts of data. It allows users to partition their data so it can be accessed in different operating systems, providing them with the freedom to configure their own data. Additionally, block storage decouples the user’s data from its original environment by spreading it across multiple environments which are better suited for serving this information when requested. This makes retrieving stored blocks faster than file-based solutions as each block lives on its own without relying on one single path back to the user's system. Block storage is usually deployed in Storage Area Network (SAN) environments and must be connected with an active server before use.
*   **_Object storage_** is a flat structure in which files are broken into pieces and spread out among hardware. Instead of being kept as files in folders or blocks on servers, the data is divided into discrete units called objects that each have their own unique identifier for retrieval over a distributed system. These objects also contain metadata describing details such as age, privacies/securities, access contingencies and other information related to the data's content. Object storage is best used for large amounts of unstructured data, especially when durability, unlimited storage, scalability, and complex metadata management are relevant factors for overall performance. To retrieve the data, the storage operating system uses the metadata and identifiers, which distributes the load better and lets administrators apply policies that perform more robust searches. Object storage offers cost efficiency since users only pay for what they use; it can easily scale up to large quantities of static data; and its agility makes it well suited for public cloud storage applications.

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgWPUcwkEYpYxIGWQW2GkBoZMyMh9e4sbOlMeEth9CneDeDoDj37f-myCXmz-5R87z5Q0hQGFj9yg3fXwfJkigJzSkQBWVgcU_zsnilqCufxb0XEIg9MBhtcHVUKlJM7bWKZ-lZpGgkQAtBjbVll8NIaa4tngy_bnIE3rpYaoDuyDgZ4kEiFvtmGXaG){: .mx-auto.d-block :} *Storage formats: Files vs. Blocks vs. Objects*{:style="display:block; margin-left:auto; margin-right:auto; text-align: center"}  

### Storage technologies

Over the past years, we've seen many storage support technologies that emerged to store and manage data: databases, data warehouses, data marts, data lakes, or recently data lakehouses. All start all with “data”. Data are everywhere, but the kind of data, scope, and use will illustrate what is the best technologie for your organization.

A **database** is a structured collection of data that can be stored in disk, memory or both. Databases come with a variety of flavors:

*   **Structured databases**: is a type of database that stores data in an organized, pre\-defined format (schemas). Structured databases are typically used to store large amounts of information and allow for quick retrieval by using queries. Structured databases can be relational or analytical:

    - **_Relational databases_**, that use relational model to organize data into tables, fields (columns) and records (rows). These tables can be linked together using relationships between different fields within each table. This type of database allows complex queries (joins) on large datasets as well as transactions such as updates, inserts or deletes across multiple related records at once (OLTP). OLTP systems typically use normalized tables with a small number of columns that contain only the most essential data needed for transactional operations.
    - **_Columnar databases_**, are designed for complex analytical tasks like trend analysis and forecasting. These types of databases (OLAP) usually have large amounts of denormalized data stored in fewer but wider tables with many columns containing detailed information about each transaction or event being analyzed. The most famous technology in this category are data warehouses.

*   **Semi-structured & Unstructured databases** also known as **Not-Only SQL (NoSQL)** do not follow the same structure as traditional SQL databases but instead allow users to store unstructured or semi-structured data without having to define fixed schemas beforehand like you would need when working with an relational database. They also provide more flexibility than their counterparts by allowing developers to quickly add new features without needing extensive modifications or migrations from existing structures due to their dynamic nature. These databases hold data in different manners and can be classified into eight categories:
    - **_Key-Value stores_**: This type of NoSQL database stores each item as an attribute name (or "key") together with its value; they provide a simple way to store large amounts of structured data that can be quickly accessed by specifying the unique key for each piece of information stored within them. Most of key-value stores are in-memory databases that stores all its data in the main memory (RAM) instead of on disk. This allows for faster access to the stored keys and provide better performance when dealing with large amounts of data.
    - **_Wide-Columns databases_**: this type is optimized for queries over large datasets—storing columns of data separately rather than rows—and typically offer faster write performance compared to row-oriented systems like relational databases due to their ability to add new columns on the fly without having to rewrite existing records or tables entirely when schema changes occur during development cycles.
    - **_Time-series databases_**: A time series database is a type of NoSQL database that is optimized for storing and analyzing time-series data. Time series data is a sequence of data points, typically consisting of timestamps and values, that are collected over time. Time series databases are used to store and analyze data such as sensor readings, stock prices, system metrics, application performance monitoring (APM), log events, website clickstreams and more.
    - **_Immutable Ledgers_**: An immutable ledger is a type of record-keeping system in which records (or "blocks") cannot be altered or deleted once they have been written. This feature makes them well-suited for applications such as financial transactions, supply chain management, and other scenarios where it's important to maintain a permanent auditable trail of all transactions. One of the most well-known examples of an immutable ledger is the blockchain, which is used as the underlying technology for cryptocurrencies like Bitcoin. In a blockchain, each block in the chain contains a record of multiple transactions, and once a block is added to the chain it cannot be altered or deleted. This provides a secure, decentralized record of all transactions that is resistant to tampering and manipulation.
    - **_Geospacial databases_**: A geospatial datastore is a type of database that specifically stores and manages geospatial data, which is data that describes the location and spatial relationships of objects and features on the Earth's surface. Geospatial datastores are used to support a variety of applications, such as geographic information systems (GIS), mapping, and spatial analysis. They allow users to perform complex spatial analysis, create maps, and make location-based decisions based on the data they contain.
    - **_Graph databases_**: A graph database uses nodes connected by edges, making it ideal for applications where relationships between entities need quick access times & efficient storage capacity.
    - **_Document databases_**: These databases store data in documents, which are organized into collections and contain key-value pairs or other complex nested structures (eg., JSON). 
    - **_Search databases_**: Search databases are databases designed specifically for searching and retrieving information based on specific criteria, such as keywords or vectors. They are commonly used in applications that require fast and efficient searching, such as e-commerce websites, online libraries, and digital archives. Search databases use various indexing and data structure techniques to optimize search performance, such as inverted indexes, B-trees, and hash tables. They often provide advanced features such as fuzzy search, synonym matching, relevance ranking, and vector similarities.

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgxrDRMBbMDsb9uRMH68CDGtJGjFXUov1AZzs3ej6JOqMDBkDq59FOkVTCiqvkuG1k01HE2k_r2Ji40NnHLMNVsDQc7jKzDNgGMq8dHy3p1_wds6VJJsBit-F01ymuRdtgzlwNVX8jXewYaLUXacYVm1YwDjE8JTlb04B5XVQI9bW3CarNzYwotvxx2){: .mx-auto.d-block :} *Difference between SQL and NoSQL databases*{:style="display:block; margin-left:auto; margin-right:auto; text-align: center"} 
  
A **data warehouse** is a structural analytical database. It tends to handle large amounts data quickly, allowing users to analyse facts according to multiple dimensions in tandem (Cube). Data warehouses have been a staple in decision support and business intelligence applications since the late 1980s. However, they are not well-suited to handle large volumes of unstructured or semi-structured data with high variety, velocity, and volume; thus making them an inefficient choice as a big data storage technology. Data warehouses can contain a wide variety of data from different sources and for different usages (ie., Enterprise Data Warehouse), it can be used for near real-time analysis and reporting on operational systems that are constantly changing or updating their data sets (ODS), or they can be split into many subsets and tailored for specific usages or operational purposes (ie., **data marts**).

About a decade ago, architects began to conceptualize a single system for storing data from multiple sources that could be used by various analytic products and workloads. This led to the development of data lakes. A **data lake** is a large, centralized repository of structured and unstructured data that can be used for any scale. 

Unlike databases or data warehouses, a data lake captures anything the organization deems valuable for future use. The data lake represents an all-in-one process. Data comes from disparate sources and serves disparate consumers. We see often data lakes working with complimentary tools like data warehouses it provides more querying options and other analytical features, and data marts because they create the other side of the spectrum: While data lakes are enormous repositories of raw data, and data marts are focused data (subject-oriented data).

The continuous need for complimentary tools, pushed many technological vendors to create a new concept that combines the best elements of data lakes and data warehouses. This new technology is called data lakehouses.

A **data lakehouse** is a combination of the low-cost, scalable storage of a data lake and the structure and management features of a data warehouse. This environment allows for cost savings while still enforcing schemas on subsets of the stored data, thus avoiding turning into an unmanageable "data swamp". For this, It comes with the following features (non-exhaustive list):

*   Data mutation: Most of data lakes are build on top of immutable technologies (eg., HDFS or S3). This means that any errors in the data cannot be corrected. To address this problem of schema evolution, data lakehouses allow two turnarounds: copy-on-write and merge-on-read.
*   ACID: support transactions to enable concurrent read and write operations while ensuring data to be ACID (Atomic, Consistent, Isolated and Durable).
*   Time-travel: The transaction capability of the lakehouse allows for going back in time on a data record by keeping track of versions.
*   Schema enforcement: Data quality has many components, but the most important one is making sure that data adheres to a schema when it is ingested. This means that any columns not present in the target table's schema must be excluded and all column types have to match up correctly.
*   End-to-end Streaming: organizations face many challenges with streaming data. One example is the out-of-order data which is solved by the data lakehouse through watermarking. It supports also merge, update and delete operations to enable complex use cases like change-data-capture (CDC), slowly-changing-dimension (SCD) operations, streaming upserts, etc

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2WokrhfA5eNxx6dvBUjCAhgsRChmB5PaZIzjnvgmNtxTkPU8x3H0JysIJHLWUVOdGFQ2wHsLlX_jtfunn4IztKe_mCNyp9y_O6b9fUhZiGLgdcgrWHuhS3HXb_1FtPwPFY383iwg8s5_No02zwZSYlkklVWJhCgd8Ue5HOdCsAgSdPXAk8qTZoRQD){: .mx-auto.d-block :} *Evolution of Storage technologies ©DataBricks*{:style="display:block; margin-left:auto; margin-right:auto; text-align: center"} 
  
### 3 - Data Processing

Processing involves transforming raw data into useful insights through analysis techniques like machine learning algorithms or statistical models depending on what type of problem needs solving within an organization's context. The goal here is not only accuracy but also efficiency since this stage requires significant computing power which could become costly over time without proper optimization strategies employed. The Data processing stage is extensively detailed [here](https://aelkouhen.github.io/2023-02-08-data-101-part-5/).

### 4 - Data Analysis

The final stage of the data journey before dissemination is data analysis. It consists of interpreting and drawing insights from processed data in order to make informed decisions or predictions about future trends. Data visualization tools can be used here to help visualize the data in a more meaningful way. 

Data analysis can be organized into six categories:

1.  **_Descriptive Analysis_**: This type of analysis answers to the question "What happened?". Descriptive analysis is used to provide an understanding of what has occurred in the past. It consists of summarizing data to describe what is going on in the dataset, by using aggregation functions (ie., counts, averages, frequencies...) It does not attempt to explain why things happened or determine any cause-and-effect relationships; its purpose is simply to give a concise overview.
2.  **_Exploratory Data Analysis_** (EDA): focuses on exploring and understanding a dataset by using visualizations like histograms, box plots, scatterplots etc., to uncover patterns and relationships between variables that may not be immediately obvious from looking at summary statistics alone. 
3.  **_Diagnostic Analysis_**: This type of analysis answers to the question "Why did it happen?". The goal of diagnostic analysis is to uncover the root cause behind an observed phenomenon. Through this type of analysis, you can identify and respond to anomalies in your data. When running diagnostic analysis, there are a number of different techniques that you might employ, such as probability, regression, filtering, and time-series analysis.
4.  **_Inferential Analysis_**: Inferential analysis is a statistical technique that uses data from a small sample to make inferences about the larger population. It relies on the central limit theorem, which states that given enough samples of random variables, their distribution will tend towards normal and can be used to estimate parameters in the underlying population with an associated measure of uncertainty or standard deviation. To ensure accuracy, it is important for your sampling scheme to accurately represent the target population so as not to introduce bias into your results.
5.  **_Predictive Analysis_**: This type of analysis answers to the question "What is likely to happen in the futur?". Predictive Analysis uses statistical techniques like regression models and machine learning algorithms to predict future events or outcomes based on past (historical) or current patterns and trends in the dataset being analyzed. This allows organizations to plan ahead and make informed decisions based on their forecasts; for example, if sales are predicted to go down during summer months due to seasonality factors, they may choose to launch promotional campaigns or adjust spending accordingly. It can also help identify potential risks associated with certain decisions or actions taken by an organization before they happen so that appropriate measures can be taken proactively rather than reactively after something has already gone wrong.   
6.  **_Prescriptive Analysis_**: This type of analysis answers to the question "What is the best course of action?". Prescriptive analysis takes predictive analysis one step further by providing recommendations for how best to act upon those predictions. It looks at what has happened, why it happened, and what might happen in order to determine what should be done next. By combining insights gained through the other kinds of analyses (list above) with advanced modeling methods such as artificial intelligence (AI) and optimization algorithms, prescriptive analysis helps organizations make better informed decisions about their operations while minimizing risk exposure.
    
![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEil7nAjvno-_wj2AoNXKNO0G_hu9Ps9XuBe-mjsEliWYGyrX_-5u3wiBYyb9q6h_H19ng739gCiQqnkLuhmgiTNcNqz61cEB-9ZYjbp_0fYiPTEBrwdCkrzCgDp4rfxWveQ2WZ6z4k6m1SFq_ikEm4ElkqsT0zXJfB9t4iiVUNIT42cqZpeC2IU4dKc){: .mx-auto.d-block :} *Data analytics continiuum*{:style="display:block; margin-left:auto; margin-right:auto; text-align: center"} 

Finally, when curated data passe through these bunch of analyses, the outcomes (insights) can be: shared with stakeholders or other interested parties. This could be done through reports, presentations, dashboards etc., depending on what kind of information needs to be shared and who it’s being shared with. Curated data and/or insights can also be shared "as-is" for further purposes such as feature stores for machine learning algorithms, data monetization, API, etc.

## Data Chain of Value

Remember the data roles we introduced in [part 2](https://aelkouhen.github.io/2023-01-24-data-101-part-2/) of this series. Let's put every role in the data journey perpective. We can observe that every one has predefined tasks, skills, and interface contracts with the other ones. Thorough the data journey landscape, the data value increases thanks to this synergy. Throughout the process, from one end of the value chain to another and back again, there should be constant feedback between producers and stakeholders.

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjkZGn38BoudzHV0HRD_fABWFPVyrE8l-n2EfMQvQVC2m72A_fLF45bY4WsdYvJd4xeR0vl36HQjnz2hsjKUX8x2ETyqDuSPUsqtT93HYkqd6P8xyUoxZ1_K10mjOyh0Lg0mxBYTB6xRrak2jtty6rwsVNBN-th9Xrk48YYCOeoElqGA67ZPEbrC7c-){: .mx-auto.d-block :} *Data roles in the Data chain of value*{:style="display:block; margin-left:auto; margin-right:auto; text-align: center"} 
  
The platform engineer builds the infrastructure of data platform for other involved data professionals to use.

*   The data engineer leverages the data platform infrastructure to build and deploy pipelines that extract data from external sources, clean it, guarantee its quality, and pass it on for further modeling. If more complex processing is needed, then the ownership of a data engineers would shift downstream accordingly.
*   The data analyst communicates with business stakeholders to create an accurate and intuitively usable Data Model used for subsequent analyses. They utilize this modeled data to conduct Exploratory Data Analyses (EDA) and Descriptive Analysis in order to answer business related questions with the help of dashboards and reports.

MLOps engineers build out the infrastructure of an ML platform which can be used by both Machine Learning engineers and data scientists alike.

*   Data scientists leverage datasets present in data platform to explore their predictive insights and build Machine Learning Models. In more mature organizations, you will find feature stores as well.
*   Machine Learning engineers leverage ML platform capabilities to deploy ML models built by data scientists ensuring MLOps best practices. Communicates back with MLOps engineers to align on and help improve and industrialize ML platform capabilities.

{: .box-warning}
**Caution:** Being further upstream the Data Value Chain does not mean that your value is less. Quite the opposite - any mistake upstream multiplies the impact of the downstream applications making the upstream roles have the most impact on the final value.

## Summary

In this article, we introduced the concept of data journey (data value chain), and how one can use modern technological tools to enable organizations and people to be more data-driven. Thus, we detailed the different stages of this landscape and observed that, for each of them, a deep architectural analysis is required to choose the right technology for the right purpose. 

We highlighted the different way to extract data, the different options of storing it and how data can be retained and used without turning the storage area into an unmanageable "data swamp". For this we overviewed the evolution of storage technologies from 80's of the last century to the few last years.

We discovered the types of analysis that we can perform on data. In some ways, data analysis is a bit like a treasure hunt; based on clues and insights from the past, you can work out what your next move should be. With the right type of analysis, all kinds of businesses and organizations can use their data to make smarter decisions, invest more wisely, improve internal processes, and ultimately increase their chances of success. 

Finally, we put data team roles in the data journey landscape. We observed that data value increases thanks to the interactions between each one of the data team. However, we want to stress that upstream activities in the data value chain does not mean less value. Every stage in the data journey has an immediate impact of the downstream stages and thus of the final value.

## References

*   https://hevodata.com/learn/incremental-data-load-vs-full-load/
*   https://www.databricks.com/blog/2019/08/14/productionizing-machine-learning-with-delta-lake.html
*   https://dzone.com/articles/data-lake-governance-best-practices
*   https://www.redhat.com/en/topics/data-storage/file-block-object-storage
*   https://www.trifacta.com/blog/from-raw-to-refined-the-staging-areas-of-your-data-lake-part-1/
*   https://www.healthcatalyst.com/insights/four-essential-zones-healthcare-data-lake
