---
layout: post
title: Data 101 - part 5
subtitle: Data processing
thumbnail-img: https://www.esds.co.in/blog/wp-content/uploads/2019/05/Digital-Solutions.gif
share-img: https://www.esds.co.in/blog/wp-content/uploads/2019/05/Digital-Solutions.gif
tags: [Parquet,ORC,data formatting,CSV,data lineage,JSON,AVRO,normalized data,data preparation,data quality,data cleaning,XML,denormalized data,join types,data discovery,data transformation]
comments: true
---

Data Processing involves transforming raw data into useful insights through analysis techniques like machine learning algorithms or statistical models depending on what type of problem needs solving within an organization's context.

Data processing is the core of any data architecture and we've seen in the last posts, that choosing the right data architecture should be a top priority for many organizations. The aim here is not only accuracy but also efficiency since this stage requires significant computing power which could become costly over time without proper optimization strategies employed.

In this post, we will cover data processing with the different stages that compose it. We will see that while data is going further in these stages, it gains in value and can offer better insights for decision-making.

## 1 - Data preparation

Once the data is collected, it then enters the data preparation stage. Data preparation, often referred to as “pre-processing” is the stage at which raw data is cleaned up and organized for the following stages of data processing. During preparation, raw data is diligently checked for any errors. The purpose of this step is to discover and eliminate bad data (redundant, incomplete, or incorrect data) and begin to create high-quality data for the best data-driven decision making.

76% of data scientists say that data preparation is the worst part of their job, yet a mandatory task to perform carefully. Efficient, accurate business decisions can only be made with clean data.

Data preparation helps catch errors and raise alerts about data quality before processing. After data has been moved from its original source, these errors become more difficult to understand and correct. Data preparation allows also to produce top-quality data. Top-quality data can be processed and analyzed more quickly and efficiently leads to accurate business decisions.

### Data Discovery

The data preparation process begins with finding the right data. Either added ad-hoc or came from an existing data catalog, data should be gathered and its attributes discovered and shared in a data catalog. The first step of data preparation is thus data discovery (or data cataloging).

According to Gartner, Augmented Data Catalogs 2019: “A data catalog creates and maintains an inventory of data assets through the discovery, description, and organization of distributed datasets. The data catalog provides context to enable data stewards, data/business analysts, data engineers, data scientists and other lines of business (LOB) data consumers to find and understand relevant datasets for the purpose of extracting business value. Modern machine-learning-augmented data catalogs automate various tedious tasks involved in data cataloging, including metadata discovery, ingestion, translation, enrichment and the creation of semantic relationships between metadata.’’

Data is a valuable asset, but its full potential can only be unlocked when users are able to comprehend and transform it into meaningful information. In the era of big data, organizations cannot afford to leave their business users dependent on IT professionals or data analysts for help in understanding large volumes of generated data. It serves as a single source of reliable information that gives viewers insight into all available organizational assets; making it essential for businesses who wish become more informed with their decisions through leveraging relevant insights from collected datasets faster than ever before possible. 

Data catalogs focus on organizing different types of assets within one platform while connecting related metadata together so that all consumers may easily search through defined categories which will ultimately lead them towards smarter decision-making processes without having any prior knowledge about the underlying dataset itself.

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjObtG5LlSNI7biAYsZTo6ZCarMpntVkLqvF44htA5aodQ2VNCgWHpeQJdoifwkEP5NxwGTVqW_kAoN0PN0vhI_I635J756xkZT1Rkko3KQrpWTK_RKJ5sjmxUBvEWKoLvRRfwEx_-hnY7FLRXjRE9fCHLxkiDK7LpLEVoyoCP_iG7HCqfQ6l_u6GGJ/w531-h212/data-catalog.png){: .mx-auto.d-block :} *Data catalog*{:style="display:block; margin-left:auto; margin-right:auto; text-align: center"}

### Data Cleaning

Data cleaning (aka data cleansing or data scrubbing) is a critical component of the preparation process, as it eliminates faulty information and fills in any gaps. This includes getting rid of unnecessary details, remove duplicates and corrupted/incorrectly formatted data, correcting outliers, filling empty fields with appropriate values, and masking confidential entries. After this step has been completed successfully, validation must occur to ensure that no errors have occurred during the cleansing procedure. If an issue arises at this stage then it needs to be addressed before moving forward.

To establish a successful data cleaning process, your organization should begin by outlining the techniques that will be used for each type of data. For instance you can follow these steps (non exhaustive) to map out a framework for your organization.

*   Remove duplicate or irrelevant observations : To remove unwanted observations from a dataset, one should first identify and eliminate any duplicate or irrelevant observations. This can be done by examining the data for similarities between records that may indicate duplication, as well as filtering out observations which do not fit into the specific problem being analyzed. Doing so will help to create an efficient dataset with only relevant information that is more manageable and performant when it comes time to analyze the data.
*   Fix structural errors: Structural errors occur when data is measured or transferred and strange naming conventions, typos, or incorrect capitalization are noticed. These inconsistencies can lead to mislabeled categories or classes; for instance, both “N/A” and “Not Applicable” may appear but should be treated as the same category.
*   Filter unwanted outliers: Occasionally, a single observation may stand out from the rest of the data and not seem to fit in with what is being analyzed. If there is an acceptable reason for eliminating it, such as incorrect entry of information, this can improve the accuracy of your results. However, outliers could also be significant; they might confirm or disprove theories you are exploring. Therefore, before discarding any unusual values that appear in your dataset, make sure to investigate their validity first. If they turn out to be irrelevant or erroneous then consider removing them from consideration.
*   Handle missing data: Missing data cannot be overlooked as many algorithms will not accept it. To address this issue, there are a few potential solutions; however, none of them is perfect. One option is to remove observations that contain missing values but doing so can lead to the loss of information. Another approach would involve filling in the gaps with assumptions based on other entries which could also compromise accuracy and integrity. Lastly, one might consider altering how the data is used in order to effectively work around null values without sacrificing its quality or reliability.

In the table bellow, you can find the most frequent data issues, with some example and how you can deal with them.

| Data Quality Issue | Description                                                                                                             | Examples                                                                                                                     | Solutions                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| ------------------ | ----------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Missing Data**       | Reason of Missing Values :<br>1.Information not collected from the source,<br>2.Attribute not applicable to all cases.  | Empty Fields                                                                                                                 | The solutions applied will depend on the amount of missing data, the attributes with the missing data and category of missing data :<br>\- Drop the observations with missing data (remove the Rows)<br>\- Fill in the missing data (Mode for Categorical data, Means or Median for Numerical Values)<br>\- Ignore the attribute with missing data (remove the Columns)                                                                                                                                                            |
| **Outliers**          | Data objects having characteristics that are considerably different than most of the other data objects in the dataset. | Having Millions while most of data values are less than 10.                                                                  | The solutions applied will depend on the Outlier nature :<br>\- A measurement error or data entry error, correct the error if possible. If you can’t fix it, remove that observation because you know it’s incorrect.<br>\- Not a part of the population you are studying (i.e., unusual properties or conditions), you can legitimately remove the outlier.<br>\- A natural part of the population you are studying, you should not remove it. Analyses the origin of the value, and consider the trade-offs to keep the outlier. |
| **Noise**              | Refers to modification of original values with additional meaningless information.                                      | Multiple Rows are filled by default values.                                                                                  | Drop Noise                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| **Contaminated Data**  | All the data exist but in the wrong field, or are inaccurate                                                            | Phone number in the Name Field                                                                                               | Data Standardization                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| **Inconsistent Data**  | Discrepancies in the same information across the dataset                                                                | \- Same person with multiple name spelling<br>\- Discrepancies in the unit of measures (Euros and Dollars in the same field) | Data Standardization                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| **Invalid Data**       | Data exists but meaningless for the context                                                                             | Inventory available units displaying a negative value                                                                        | Data Standardization                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| **Duplicate Data**     | Reason of Duplicates : Merge of Data from Heterogenous Sources                                                          | Same person present in multiple rows                                                                                         | Drop Duplicates                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| **Datatype Issues**    | Data exists but with the wrong Data archetype                                                                           | Having Dates or Numbers in String Fields                                                                                     | Type Inference                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| **Structural Issues**  | Data structure is not as expected                                                                                       | Expecting 10 fields and getting only 3<br>Expecting a specific Structure Order and getting a different order.                | Data Discovery (validate against data catalog)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |  

At the end of the data cleaning process, you should be able to answer these questions as a part of basic validation:

*   Does the data make sense?
*   Does the data follow the appropriate rules for its field?
*   Does it prove or disprove your working theory, or bring any insight to light?
*   Can you find trends in the data to help you form your next theory?
*   If not, is that because of a data quality issue?

False conclusions because of incorrect or “dirty” data can inform poor business strategy and decision-making. False conclusions can lead to an embarrassing moment in a reporting meeting when you realize your data doesn’t stand up to scrutiny. Before you get there, it is important to create a culture of quality data in your organization. To do this, you should document the tools you might use to create this culture and what data quality means to you.

Once prepared, the data can be stored or leveraged to a trusted storage zone (in a multi-tier storage) clearing the way for processing and analysis to take place.

## 2 - Data formatting

The clean data is then entered into its destination (perhaps a CRM like Salesforce or a data warehouse like Redshift), and should be formatted into a format that it can understand and efficient to process. Data formatting is the first stage in which cleaned data begins to take the form of usable information.

In data processing, there are various file formats to store datasets. Each format has its own advantages and disadvantages depending on the use cases it is intended for. It is important to understand their specificities when selecting a particular format type. For instance, CSV files are easy to comprehend but lack formalism; JSON is preferred in web communication while XML may also be used; AVRO and Protocol Buffers work well with streaming processing; ORC and Parquet offer improved performance in analytics tasks due to column storage capabilities. Furthermore, Protocol Buffers form the basis of gRPC which powers Kubernetes ecosystem applications.

### Formatting Considerations

Many considerations have to be taken into account for choosing a data format and the underlying technology for its storage (aka file format). In this section, I'll present and describe the most popular and representative data formats regarding their main characteristics. For example, in regards to storage mode, queries types, processing usages, etc. The goal here is to provide a way to compare most popular data formats depending on what you want to perform in this stage. The main characteristics we retained for comparison are:

*   **Text vs. binary file format**: While both binary and text files contain data stored as a series of bits (binary values of 1s and 0s), the bits in text files represent characters (ASCII code), while the bits in binary files represent custom data. Text-based file formats are advantageous in that they can be read and modified by the end user with a text editor. Furthermore, these files tend to have smaller storage footprints due to compression. On the other hand, binary file formats require specialized tools or libraries for creation and consumption; however, their data serialization is optimized for better performance while processing data.
*   **Data type**: Some formats do not permit the declaration of multiple data types. These types are divided into two categories: scalar and complex. Scalar values contain a single value, such as integers, booleans, strings or nulls; whereas complex values consist of combinations of scalars (e.g., arrays and objects). Declaring type is beneficial in that it allows consumers to differentiate between numbers and strings, for example, or distinguish a null from the string "null". When encoded in a binary form this distinction results in significant storage optimization: For example, the string “1234” requires four bytes while boolean 1234 only needs two bytes.
*   **Schema enforcement**: The schema store the definition of each attribute and its types. Unless your data is immutable, you will need to consider schema evolution in order to determine if the structure of your data changes over time. Schema evolution enables updating the schema used for writing new data while still being compatible with the old schemas that were previously used (backwards compatibility).
*   **Row vs. column storage**: In row-based storage, data is organized and stored in rows. This architecture makes it easy to add new records quickly and facilitates simultaneous access or processing of an entire row of data. It is commonly used for Online Transactional Processing (OLTP) systems that process CRUD queries at a record level. OLTP transactions are usually very specific tasks involving one or more records, with the main emphasis being placed on maintaining data integrity in multi-access environments while achieving high numbers of transactions per second. Inversely, column-based storage is most useful when performing analytics queries which require only a subset of columns examined over very large data sets. This approach, known as OLAP (OnLine Analytical Processing), has played an important role in business intelligence analytics and Big Data processing. By storing data in columns instead of rows, it avoids the excessive time needed to read irrelevant information across a dataset by ignoring all the data that doesn’t apply to a particular query. It also provides greater compression ratio due to aligning similar types of data and optimizing null values for sparse columns. As an example, if you wanted to know the average population density in cities with more than one million people using row-oriented storage your query would access each record from the table (all its fields) while columnar databases reduce this amount significantly improving performance times.

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrDH33UTOe7ZVldZR0jz01-p_daFWY66aAw9-182Ekbj0qJ2WliOYcvIE_PnUkpcojXWiUYgvJPm5jUov-_Dci6emJzcBA2rfEkeTXbpkFZAXUn_SasY70zjpm8PS9o7LgCtpy_27BiIQy36csIeqV3pdoD910yIdy7aURK0bG3MaF0Kz-qJ3B6Sct/w424-h155/row-column-properties.png){: .mx-auto.d-block :} *Grouping records by properties*{:style="display:block; margin-left:auto; margin-right:auto; text-align: center"}

*   **Splittable**: In a distributed system like Hadoop HDFS, it is essential to have a file that can be broken down into multiple parts. This allows for the data processing in the system to be distributed among these chunks, thus being able to scale the processing to infinite. Being able to start reading from any point within a file makes full use of Hadoop's distributed capabilities. If the format of the file does not permit splitting, then users must manually divide their files into smaller pieces to enable parallelism.
*   **Data Compression**: A system's performance is largely determined by its slowest components, which are often the disks. Compression can reduce the size of datasets stored and thus decrease read IO operations, as well as speed up file transfers over networks. Column-based storage offers higher compression ratios and better performance than row-based storage due to similar pieces of data being grouped together; this is especially effective for sparse columns with many null values.When comparing compression ratio, one must also consider the original size of each respective file - e.g., it would be unfair to compare JSON versus XML without taking into account that XML tends to have a bigger initial footprint than JSON does. There are various types of compression algorithms available depending on factors such as desired level of compression, speed/performance requirements, whether or not they're splittable, etc.
*   **Data velocity**: Batch processing involves reading, analyzing and transforming multiple records at once. In contrast, stream processing works in real-time to process incoming records as they arrive. It is possible for the same data set to be processed both through streaming and batch methods. For example, financial transactions received by a system could be streamed instantly for fraud detection while also being stored periodically in batches so that reports can be prepared from them later on. To achieve this goal, one or two file formats might need to exist between the format of the data arriving over wire and what is eventually stored in Data Warehouse/Data Lake systems.
*   **Ecosystem and Integration**: It is advisable to adhere to the conventions and customs of an ecosystem when selecting between multiple options. For instance, Parquet should be used on Cloudera platforms while ORC with Hive should be preferred for Hortonworks ones as it will likely result in a smoother integration process and more helpful documentation/support from the community.

### File formats

We are going to focus on some of the more common data formats, providing examples that will help data engineers retrieve their desired information. We cannot examine all available formats due to the vast number of formats available. Instead, we will tackle several of the more common formats, providing adequate examples to address the most common data retrieval needs:

#### 1- Delimited Flat Files (CSV, TSV)

Comma-separated values (CSV) and Tab-separated values (TSV) are text file formats that use newlines to separate records and an optional header row. They can be easily edited by hand, making them highly comprehensible from a human perspective. Unfortunately, neither CSV or TSV do not support schema evolution; even though the header may sometimes serve as the data's schema, this isn't always reliable. Furthermore, in their more complex forms they cannot be split into smaller chunks without losing relevance due to the lack of any specific character on which splitting could be based upon.

The differences between TSV and CSV formats can be confusing. The obvious distinction is the default field delimiter: TSV uses TAB, CSV uses comma. Both use newline as the record delimiter.

The CSV format is not fully standardized, as its field delimiters can be a variety of characters such as colons or semicolons. This makes it difficult to parse when the fields themselves contain these same quotes and even embedded line breaks. Furthermore, data in each field may also be enclosed by quotation marks which further complicates parsing due to potential irrelevant structure regarding sentence beginnings and endings or null values. In contrast, TSV typically uses either tabulation or commas for delimiting without an escape character; this simplifies distinguishing between fields and thus facilitates easier parsing than with CSV files. For this reason, TSV is mainly used in big data cases where ease of parsing matters most. 

This file formats are very popular for data storage and transfer. they have excellent compression ratios, making them easy to read and supports both batch and streaming processing. However, neither CSV not TSV do not support null values as they are treated the same as empty values, there is no guarantee that files will be splittable or have schema evolution support either. 

Despite these drawbacks, delimited text formats remain ones of the most widely supported formats due to their simplicity with many applications such as Hadoop, Spark, Kafka etc.

#### 2- JSON (JavaScript object notation)

JSON is a text-based format that can store data in various forms, such as strings, integers, booleans, arrays, objects key-value pairs and nested data structures. It's lightweight and human-readable while also being machine readable; making it an ideal choice for web applications. JSON has widespread support across many software platforms and its implementation is relatively straightforward in multiple languages. Furthermore, the serialization/deserialization process of JSON natively supports this type of data storage.

The main disadvantage of this format is that it cannot be split. To overcome this, JSON lines are often used instead; these contain multiple valid JSON objects on separate lines, each separated by a newline character **\\n**. Since the data being serialized in JSON does not include any newlines, we can use them to divide the dataset into smaller chunks. 

JSON is a compressible text-based format used for data exchange, and supports batch and streaming processing. It stores metadata with the data and allows schema evolution. However, it lacks indexing as many other text formats. JSON is widely used in web applications such as NoSQL databases like MongoDB or Couchbase, GraphQL API's etc.

#### 3- XML (Extensible Markup Language)

XML is a markup language created by W3C that provides both humans and machines with the ability to read encoded documents. It enables users to create complex or simple languages, as well as establish standard file formats for exchanging data between applications. Similar to JSON, XML can be used for serialization and encapsulation of information; however its syntax tends to be more verbose than other text-based alternatives when viewed by human readers.

XML is a verbose file format that supports batch and streaming processing, stores metadata along with the data, and allows for schema evolution. It has drawbacks such as not being splittable due to opening/closing tags at the beginning/end of files; redundant syntax leading to higher storage and transportation costs when dealing with large volumes of data, and difficulties in parsing without an appropriate DOM or SAX parser. Despite these downsides, XML still maintains its presence in many companies due to its long history.

#### 4- AVRO

Apache Avro is a data serialization framework developed as part of Apache's Hadoop project. It uses row-based storage to serialize data, storing the schema in JSON format which makes it easy for any program to read and interpret. The actual data itself is stored in binary form, making it compact and efficient. A key feature of Avro is its ability to handle schema evolution; metadata about each record can be attached directly into the data itself. Additionally, Avro supports both dynamic typing (no need to define types in advance) as well as static typing (types are defined before use).

Avro is splittable, compressible and supports schema evolution. It has a compact storage format which makes it fast and efficient for analytics. Avro schemas are defined in JSON making them easy to read and parse, while the accompanying data allows full processing on the data. The downside of Avro is that its data cannot be read by humans, nor does it have integration with every language. However, due to being widely used in many applications such as Kafka or Spark, as well as being an RPC (Remote Procedure Call), Avro remains one of the most popular formats for exchanging information between systems today.

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjT5bXENly_qqgBPHCJtIRGsAPI2ioxfiVEU9gjObx1IXNLdf8siIeKCvS_wuv8B6DLGjNU2uM14tfOi4d7WOHYI8KA--G2gfY-y0VcehakZCTT28-7CkyC_zpv_men7r7ke0nrqRwWcbanRvkMD1lBOOCq5FhOoDScOnU0hdLCHx_QDwO-2qZrkxcX/w519-h209/avro.png){: .mx-auto.d-block :} *Structure of AVRO format*{:style="display:block; margin-left:auto; margin-right:auto; text-align: center"}

#### 5- ProtoBuf (Protocol Buffers)

Protobuf (short for Protocol Buffers) is a binary data serialization format developed by Google in 2008. It is used to efficiently encode structured data in an extensible and language-neutral way, making it ideal for applications such as network communication or storage of configuration settings. Protobuf messages are compact and can be parsed quickly, allowing them to be transferred over networks faster than other formats like JSON or XML. Unlike Avro, the schema is required to interpret the data. Protocol Buffers format (Protobuf) has advantages such as being fully typed, supporting schema evolution and batch/streaming processing, but it does not support Map Reduce or compression.

#### 6- ORC (Optimized Row Columnar) 

ORC (short for Optimized Row Columnar) format was developed by Hortonworks in partnership with Facebook. It is a column-oriented data storage system. It consists of stripes containing groups of row data and auxiliary information in a file footer. At the end of the file, a postscript stores compression parameters and the size of the compressed footer. By setting large stripe sizes (e.g., 250 MB as default), HDFS can perform efficient reads on larger files more effectively.

ORC is a highly compressible file format that reduces the size of data up to 75%. It is more efficient for OLAP than OLTP queries and used mainly for batch processing. However, it does not support schema evolution or adding new data without having to recreate the file. ORC improves performance reading, writing, and processing in Hive which makes it an ideal choice for Business Intelligence workloads.

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEih7_47G24Rd7cUFEhawrkoxA5Jl0MBXl5E1d4-HLIFgLFKMEXAVVO6kJjYhzNkPSA2V89GwN6D0D_G96vb8ALpdYqy6YYhIzktW8QTn6lnnESM7n7s3I4QJtud1TWDGQPHVOJjP5vCNZiTiqWMFtnBDERGadoOYwjeZy4g8hXqA7JV650-mf5P4URJ/w410-h389/ORC.png){: .mx-auto.d-block :} *Structure of ORC format*{:style="display:block; margin-left:auto; margin-right:auto; text-align: center"}

#### 7- Parquet

Parquet is an open source file format designed for the Hadoop ecosystem. It was created as a joint effort between Twitter and Cloudera to provide efficient storage of analytics data in a column-oriented fashion, similar to ORC files. Parquet offers advanced compression and encoding schemes which enable it to handle complex data quickly and efficiently when dealing with large volumes of information. The binary file contains metadata about its content, including the column metadata stored at the end of the file that allows for one-pass writing during creation. Furthermore, Parquet's optimization makes it ideal for write once read many (WORM) applications. 

Parquet has advantages such as being splittable, allowing better compression and schema evolution due to organizing by columns and having high efficiency for OLAP workloads. However, it can be difficult to apply updates without deleting and recreating the file again. Parquet is commonly used in ecosystems that require efficient analysis for BI (Business Intelligence) or fast read times with engines like Spark, Impala, Arrow and Drill.

Databricks created recently a new file format called Delta Lake that uses versioned Parquet files to store data. Apart from the versions, Delta Lake also stores a transaction log to keep track of all the commits made to the table or blob store directory to provide ACID transactions and to leverage the time-machine feature.

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgq4XCrUOWU165j1mIgaDB7K2NCfx6MTN6X7CoE83zWEvYmDeqcQvjllC6sCOl9dmssWka9Zbz0mKqvI7esC8pmhBypdSpJz8McioY-Xd9uiIbH_lSlLPiMmU0NME70TgH73Njw4k6HYmbnbRp28dTtVDGTLJtSHtnGkJHgqNuwzbHpUwpK79EoAlnT/w456-h377/parquet.png){: .mx-auto.d-block :} *Structure of Parquet format*{:style="display:block; margin-left:auto; margin-right:auto; text-align: center"} 
  
The selection of a file format should be based on the type of query and use cases associated with it. Row-oriented databases are typically used for OLTP workloads, while column-oriented systems are better suited to OLAP queries due to their uniform data types in each column which allows them to achieve higher compression than row storage formats. For text files, JSON line or CSV (if all data types can be controlled) is recommended over XML as its complexity often requires custom parsers. Avro is an ideal choice when flexibility and power is needed for stream processing and data exchange; however Protocol Buffers may not always be suitable as it cannot split nor compress stored data at rest. Finally, ORC and Parquet formats provide optimal performance for analytics tasks. Ultimately, choosing the right file format depends on the specific use case requirements. Hereafter, a summary table for the data formats regarding the chosen considerations:

<div class="table-wrapper" markdown="block">

| Attribute\\Format | **CSV**                 | **JSON**                    | **XML**                     | **AVRO**                             | **Protocol Buffers**   | **Parquet**         | **ORC**             |
| ---------------------- | ------------------- | ----------------------- | ----------------------- | -------------------------------- | ------------------ | --------------- | --------------- |
| Text vs. Binary        | Text                | Text                    | Text                    | metadata in JSON, data in binary | Text               | Binary          | Binary          |
| Data type              | No                  | Yes                     | No                      | Yes                              | Yes                | Yes             | Yes             |
| Schema enforcement     | No                  | external for validation | external for validation | Yes                              | Yes                | Yes             | Yes             |
| Schema evolution       | No                  | Yes                     | Yes                     | Yes                              | No                 | Yes             | No              |
| Storage type           | Row-based           | Row-based               | Row-based               | Row-based                        | Row-based          | Column-based    | Column-based    |
| OLAP/OLTP              | OLTP                | OLTP                    | OLTP                    | OLTP                             | OLTP               | OLAP            | OLAP            |
| Splittable             | Yes (simplest form) | Yes (JSON lines)        | No                      | Yes                              | No                 | Yes             | Yes             |
| Data compression       | Yes                 | Yes                     | Yes                     | Yes                              | Yes                | Yes             | Yes             |
| Data velocity          | Batch & Stream      | Batch & Stream          | Batch                   | Batch & Stream                   | Batch & Stream     | Batch           | Batch           |
| Typed data             | No                  | No                      | No                      | No                               | Yes                | No              | No              |
| Ecosystems             | Very popular        | API and web             | Enterprise              | Big Data and Streaming           | RPC and Kubernetes | Big Data and BI | Big Data and BI |

</div>

## 3 - Data transformation

Organizations generate vast amounts of data on a daily basis, but it is only useful if they can utilize it to gain insights and promote business growth. Data transformation is the process of converting and structuring data from one form into into a usable form that can support decision making processes. Data transformation is utilized when data needs to be changed so that it matches the format and the structure of the target system. This can take place in the so called data pipelines. 

There are several reasons why companies should consider transforming their data: compatibility between disparate sets of information; easier migration from source format to target format; consolidation of structured and unstructured datasets; enrichment which improves the quality of the collected info; and ultimately achieving consistent, accessible data with accurate analytics-driven predictions.

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1ImrFN_dlXOTMogpCd6B38sjAiTaWSGp35e3IU-N6fiFhEQzWFAjR7zuuHW-EUODPyWDuA-87LaO5MpPo5dhBlTqXm_29iI31XqTNcXE1mX22JB_sGRETRtWhVi5pwsIwZGu1-v-2SNdPERYKiWUlGJC-nlqDNGO5vKSG1Y5ghQ7HcuSR1QyflP28/w493-h160/data-transformation.png){: .mx-auto.d-block :} *Transforming data*{:style="display:block; margin-left:auto; margin-right:auto; text-align: center"} 

After data preparation (called also pre-processing), in which data is identifies and understood in its original source format (data discovery), a data transformation pipeline starts with a mapping phase. This involves analyzing how individual fields will be modified, joined or aggregated in order to understand the necessary changes that need to be made. The pipeline that is required to run the transformation process, is created in then using a data transformation platform or tool. Here two approaches can be followed:

*   **Code-centric tools**: analysis and manipulation libraries built on top of general-purpose programming languages (Scala, Java or Python). These libraries manipulate data using the native data structures of the programming language.
*   **Query-centric tools**: use a querying language like SQL (Structured Query Language) to manage and manipulate datasets. These languages can be used to create, update and delete records, as well as query the data for specific information.
*   **Hybrid tools**: implement SQL on top of general-purpose programming languages. This is the case of some libraries like Apache Spark or Apache Kafka that provides a SQL dialect called KSQL.

Organizations often use ETL tools (Extract-Transform-Load), where the transformation can occurs following one or many of the above approaches. In the next blog posts, I'll create few data pipelines using tools in each of these categories. 

Data transformation is a critical part of the data journey. This process can perform constructive task such as adding or copying records and fields, destructive actions like filtering and deleting certain values, aesthetic adjustments to standardize values, or structural changes that include renaming columns, moving them around and merging them together. Data transformation can be used for :

*   **Filtering**: selects a subset from your dataset (certain columns) that require transformation, viewing or analysis. This selection can be based on certain criteria, such as specific values in one or more columns, and it typically results in only part of the original data being used. Filtering allows you to quickly identify trends and patterns within your dataset that may not have been visible before. It also enables you to focus on particular aspects of interest without having to sift through all available information. In addition, this technique can help reduce complexity by eliminating unnecessary details while still preserving important insights about the underlying data structure.

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj4h1xRWfbTZ_Z5fswB1udRRnuCFRl_6_mAIeXHCzrBeTEvz6V8MrJuwaJiiIv0Umq2qqyHOovlqkEI1B1oqMXQOwVBb7SKzHCk8LmPGm7lkwAgCqQLHB2h8D8LpoMVJfntM-emHIpgsRq07a2k_NAB1lvryOxdQF0BsFG5eUePcpGuRbMS2eHRXMzO){: .mx-auto.d-block :} *Filtering a dataset: Query-centric vs. Code-centric*{:style="display:block; margin-left:auto; margin-right:auto; text-align: center"}  

*   **Enriching**: fills out the basic gaps in the data set. This process enhances existing information by supplementing incomplete or missing data with relevant context. It aims to improve accuracy, quality and value for better results.
  
 ![My image](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpktw9SqncislkBMPz5QPYQQDZAzwz5eS05dxAk-r0VdN8S2w729ezed4Qbl_ExSE-5K03WvymkM2547yrI9ovqiZ4OKQxn0EfOQaBlu4x6uSuEILaA4z9y7k8jMh8nHcQGGSW3Mci8gDol3dFRpRj9msPnHZLuSZLlpB2J5LVjr6WHR7cEPGn2gc3){: .mx-auto.d-block :} *Enriching a dataset: Query-centric vs. Code-centric*{:style="display:block; margin-left:auto; margin-right:auto; text-align: center"}

*   **Splitting**: where a single column is split into multiple,

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiixMDNIfpmsfHJJhOXRlmZ9aFG_GEsXBcyHLK8HjWkF3ii03JQvtmXRlsq8rfuUhtJ0nWwiZOVyu4gLtl7MQHBqn_W-_8t7-E0rcu9WJyYnwMXQLbHY9QKyLyMImHM5fnDl1cjiq1b4qk7ZSd9n7uCOalXJHhC-2VRqCaliURUOYm5qKxAnb-K4VRY){: .mx-auto.d-block :} *Splitting a column: Query-centric vs. Code-centric*{:style="display:block; margin-left:auto; margin-right:auto; text-align: center"} 

*   **Merging**: where multiple columns are merged into one,

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiDFwAJuBDXIRZtBMgM8yXNKIDcGuRwmzArEWg77c6XYZhYSMBYC5q_4fDHobJpvHPrrMMEr5seY5rX9SKnTiV11kWaq9VW9nrh1_4hMaFn7-v7rgT09VfNGgODoLyWeUSJyrLAhtWIgYEd36Y7aT1_JDeKw1BUwQZTuiPu2R2jzKe1Wp0NMK2ZPEHC){: .mx-auto.d-block :} *Merging two columns: Query-centric vs. Code-centric*{:style="display:block; margin-left:auto; margin-right:auto; text-align: center"} 

*   **Removing** parts of data,

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiOkCpff0nXKKqpxAnPnPf0PfRotJ85CjqjpxpKDcAuQbK_Ne74VKfDJTTcsrL7oo0kBDWrN9fGF2xThGiFfcKA95dR5nABRC2xthnmo4aHlb6jBnxIaZIk642CTioDCt2jVeJAuAfFFIzrJwrxhMA7ndFMKfGFZsI3rrQxy8meB1hMiSjwKzZT5yMe){: .mx-auto.d-block :} *Drop duplicates: Query-centric vs. Code-centric*{:style="display:block; margin-left:auto; margin-right:auto; text-align: center"}  

*   **Joining** data from different sources. A data join is when two data sets are combined in a side by side manner, therefore at least one column in each data set must be the same. This differs from a union which puts data sets on top of each other, requiring all of the columns to be the same (or have null values in a column which isn’t common across data sets).

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJRS_PyJayRgsBVmw0CA3UVwCCyylJS0UpPc_Av0FQxWeyUHQUIWu07zbpLPiFV-hJ7P3HESBJB-ZcgdYpFckcC2lgVZ119yFc3VlUUAhD2x-20Z9mUvaIkF-kLwXikkw8_xSZH1bAlh3GVv56CxD_8RmU1HKrhQ3fmSxwzir2_6Y1oo965XE8ieN5){: .mx-auto.d-block :} *Joining tables: Query-centric vs. Code-centric*{:style="display:block; margin-left:auto; margin-right:auto; text-align: center"}  
  
There are 9 types of joins: inner, outer, left outer, right outer, anti outer, anti left, anti right, self join and cross join:

1.  An inner join takes the rows of data where both sets have fields in common. Any rows with no shared values between the two datasets on their joined column won't be included.
2.  An outer join will keep all data from both data sets. Rows which are common across data sets will have columns filled from both data sets, whereas rows without commonality will fill the blanks in with NULL values.
3.  The left outer join combines two data sets, taking all the rows that are shared between them as well as any remaining rows from the set on the left. Any right-hand columns for which there is no corresponding row in the other dataset will be filled with NULL values.
4.  The right outer join combines two data sets, taking all the rows that are shared between them as well as any remaining rows from the set on the right. Any left-hand columns for which there is no corresponding row in the other dataset will be filled with NULL values.
5.  An anti outer join excludes rows which are common between datasets. This leaves only rows that are unique in each side.
6.  An anti left join excludes rows which are common between datasets and any extra rows from the right data set. This leaves only rows from the left data set filling in the columns from the right data set columns with NULL values.
7.  An anti right join is the same as an anti left join but keeps only rows from the right data set instead of the left.
8.  Cross join: returns the Cartesian product of both tables, i.e., every possible combination of rows from both tables.
9.  Self join: joining a table with itself, typically by using an alias for the same table. This can be used to find relationships within the data in a single table.

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh1BVJ3mSyR8wKKmZhMgCGW3XGH9jxQ0Gw3tkvsuFkckQWPLg-BxR7KySZ3XDvcDWDUYqI39p7CyG14glQbmUrzNERM0jJcYdzIsPeBXsTTfIQMV8WcSy5U3tbdtfJfiuYx2_qUZU4BWD2VZwL0yVqmeCcMvIYHnkDqHS7MIphrsu03uRk9Loz254xZ){: .mx-auto.d-block :} *Common Join types*{:style="display:block; margin-left:auto; margin-right:auto; text-align: center"} 

*   Aggregating: provides data summaries or statistics such as averages and sums. These calculations are done on multiple values to return a single result. However the count function does take into account NULL values when performing its calculation.

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgP6Ox2dvaT1Q2TPoBOHz69W5F6y8MWf0KfQkGPPoKwHDM8Tew1Q4GygUxAaz8DjQy_scMpX0SbahAMJg8fnY_3u-YI7JgmngMQVP_VI59oE9bNcmFhWkiHaUzqcwj1XCO-go78CORx-OQ9AyGlPa_1JEuztKolkkcf-ScNm9AvzMCV6rIQhDtWZMno){: .mx-auto.d-block :} *Aggregation: Query-centric vs. Code-centric*{:style="display:block; margin-left:auto; margin-right:auto; text-align: center"} 

*   Derivation: which is cross column calculations. A derived field is a column whose value is derived from the value of one or more columns. The value is determined by a formula that can use constant values or values in other columns in the same table field row.

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjs0eOnB_mpb8DoKsMhODd4sGGWV8PEQGXFbMshZ6kSGWJtsqmGZPffT6_EC-gMLxDnWcFhwIHxKLPn1FgXRKHM18f_VxAlFA-9JSmIxstC4m5jAvENsIFTN7oinBpaqYW2JbQCrg91Z9pRFMnIIm-YX1_0gSU9gm6iMx6Y_ydvYkSvUrkwVYT98017){: .mx-auto.d-block :} *Derivation: Query-centric vs. Code-centric*{:style="display:block; margin-left:auto; margin-right:auto; text-align: center"}  

*   Pivoting: which involves converting columns values into rows and vice versa. You can use the PIVOT and UNPIVOT relational operators to change a table-valued expression into another table. PIVOT rotates a table-valued expression by turning the unique values from one column in the expression into multiple columns in the output. And PIVOT runs aggregations where they're required on any remaining column values that are wanted in the final output. UNPIVOT carries out the opposite operation to PIVOT by rotating columns of a table-valued expression into column values.

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgk3ZlcjWzhe1X39L4SVHL7mpwhL64qyxnAAYu64Ep4cUZbN57N7SDe8LNF2w_FQvufspBamR7v4nT1FGHkasog8XW1EZFAE4bY49y_l-xryrqODsj0b93yr_9JyTE2FeoYVL0xeL0f0wyW4Ard_CtMn4KX5gHiAoLhVc_rK2Pxt6XwtRD1Qg-YQP7C){: .mx-auto.d-block :} *Table pivoting*{:style="display:block; margin-left:auto; margin-right:auto; text-align: center"}  

*   Sorting: ordering and indexing of data to enhance search performance. Sorting data is a useful technique for making sense of research information. It helps to organize the raw figures into an understandable form, allowing it to be analyzed and visualized more easily. This can involve sorting all records in their original state or creating aggregated tables, charts or other summarized outputs from them. In either case, this process makes it easier to interpret what the data is conveying.

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjzPzabEgw1qCLrrO0apMBQsCtdmCFUDRGZLfkUZ_Zh1MKdHnf-izbp2qon9PpzONo01geJb5MUs04iIB6YZq8rKBrWSCZ5McS8hnflzhdtTsHSZAeO2ZEj1OnvLh9l3ShCIWibAKAvCsfyjDASicK9XkC7u9tcf4wVqWoDlesFww2cjSnxSoNXX8Pm){: .mx-auto.d-block :} *Sorting by a column: Query-centric vs. Code-centric*{:style="display:block; margin-left:auto; margin-right:auto; text-align: center"}  

*   Scaling: normalization and standardization that helps in comparing dissimilar numbers by putting them on a consistent scale. feature scaling is an essential step in the preprocessing of data for machine learning. Without it, algorithms that rely on distance measures between features may be biased towards numerically larger values and produce inaccurate results. Scaling helps to ensure all features are given equal weight when computing distances or performing other calculations. normalization and standardization that helps in comparing dissimilar numbers by putting them on a consistent scale.

Normalization or Min-Max Scaling is used to transform features to be on a similar scale. This scales the range to \[0, 1\] or sometimes \[-1, 1\]. Normalization is useful when there are no outliers as it cannot cope up with them. Usually, we would scale age and not incomes because only a few people have high incomes but the age is close to uniform. 

Standardization or Z-Score Normalization is the transformation of features by subtracting from mean and dividing by standard deviation. This is often called as Z-score. Standardization can be helpful in cases where the data follows a Gaussian distribution. However, this does not have to be necessarily true. Geometrically speaking, it translates the data to the mean vector of original data to the origin and squishes or expands the points if standard deviation is 1 respectively. Standardization does not get affected by outliers because there is no predefined range of transformed features.

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEixrD-oHvhSqruMDmVkHBG5tzS557vOXl1rxXM2t_yDFISqU_LVGIrwSe7snlN8vua5zMa5wEWbmTIDL7Tz3Q5_g-EZb9TK9SgnMoYXIPKlnf_z14pkajmXkly-ehE4UJ_TkfWI66e9YzABLVXk2QVsfrEeGm3FlC4SELGo9RUbgHDvl_ZCY_LsKzHM){: .mx-auto.d-block :} 
  
*   Data structure normalization is the process of organizing data into a logical structure that can be used to improve performance and reduce redundancy. This involves breaking down complex datasets into smaller, more manageable pieces by eliminating redundant information or consolidating related items together. Normalization also helps ensure consistency in how data is stored and accessed across different systems. Data denormalizing is the opposite of normalizing; it takes normalized data structures and combines them back together so they are easier to query or analyze as one unit,
*   Denormalization is the inverse process of normalization, where the normalized schema is converted into a schema which has redundant information. Denormalized data often contain duplicate values which increase storage requirements but make querying faster since all relevant information for a given task may be found within one table instead of having to join multiple tables together first before running queries on them.

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQO0MGzDlKl-5UC3BkaMhADWptFOBOexWLANY3rQjNeLmc_Vhgm7wPPL7_Ylejg9pdsb6-N_KBBmHOLSl2-fYWG--afAJjiddjte8c1-3frCpso9QJwPdkmSzRYcLXTo8q1ZjIJu0aJ4jcB2kYz47rUyTxu-c1pEj8ca18YWLdMU3XW-XfRAwICkFs){: .mx-auto.d-block :} *Normalization vs. Denormalization*{:style="display:block; margin-left:auto; margin-right:auto; text-align: center"} 
  
Structure Normalization is used in OLTP systems to make insert, delete and update operations faster. Denormalization on the other hand is used in OLAP systems to optimize search and analysis speed. Normalization ensures data integrity by eliminating redundant data while denormalizing makes it harder to retain this integrity as more redundant information may be stored. Furthermore, normalization increases the number of tables and joins whereas denormalizing reduces them. Disk space optimization also differs between these two processes with normalized tables using less storage than their denormalized counterparts due to duplicate records being eliminated.

*   Vectorization: which helps convert non-numerical data into number arrays that are often used for machine learning applications,
*   And much more...

## 4 - Data Lineage

As data sources are added, modified or updated, it is increasingly important to keep track of the relationships between and within datasets. This can range from a simple column renaming to complex joins across multiple tables sourced from different databases that may have undergone several transformations. Lineage provides traceability for understanding where fields and datasets come from as well as an audit trail for tracking when changes were made, why they were made and by whom. Capturing all this information in a Data Lake environment is difficult even with purpose-built solutions. Lineage requires aggregating logs at both transactional (who accessed what) and structural/filesystem levels (relationships between datasets). It involves any batch processing tools such as MapReduce or Spark but also external systems like RDBMSs which manipulate the data.

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgilaB3ptGqhCyJ1mU5Uo5jDHEPVSKKBJplBVkdCb5Nyvm2-CE2BrEp8IuGurE8U9gndCdnLHRNb75QHSOpwvTJ2wPbxblbqKUFWcz5o5fSQDPpjfjNicNiT6PsPv3aku4GS6IT9fpx_NgUAxGDYU0CozVPY6b4pbP-KCSVJM696CHhjYUQZBLzIHoS){: .mx-auto.d-block :} *Data lineage diagram*{:style="display:block; margin-left:auto; margin-right:auto; text-align: center"} 

Tracking data lineage is essential to ensure that strategic decisions are based on accurate information. Without it, verifying the source and transformation of data can be costly and time-consuming. Data lineage helps users confirm that their data has been sourced from a reliable source, transformed correctly, and loaded into the intended destination.

Here are a few common techniques used to perform data lineage on strategic datasets:

**1- Pattern-Based Lineage**

This technique uses metadata to trace the lineage of data without analyzing the pipeline implementation. It looks for patterns in tables, columns and business reports by comparing similar names and values between datasets. If a match is found, then these two elements are linked together on a data lineage chart to show how they relate. This method can be used to quickly identify relationships between different stages of the same dataset's lifecycle.

The primary benefit of pattern-based lineage is its versatility; it does not rely on any specific technology and can be used across different databases, such as Oracle, MySQL. However, this approach may lack accuracy since connections between datasets could potentially go undetected if the data processing logic is hidden in pipeline implementation rather than being visible in human-readable metadata.

**2- Lineage by Data Tagging**

This technique relies on the tags that transformation engines add to the processed data. By tracking these tags from start to finish, it is possible to discover lineage information about the data. However, for this method to be effective, there must be consistent usage of the same transformation tool across all data movement and an understanding of its tagging structure. Furthermore, since any external sources generated outside of this system are not tagged by default, they cannot benefit from such lineage tracing via tagging alone; thus making it suitable only for closed systems where no other external sources exist.

**3- Self-Contained Lineage**

Some organizations have a sole data environment that provides storage, processing logic, and master data management (MDM) for central control over metadata. This self-contained system can inherently provide lineage without the need for external tools. However, it will be unaware of any changes or updates made outside this controlled environment.

**4- Lineage by Parsing (reverse engineering)**

This is the most advanced form of lineage. It automatically reads the logic used to process data, to trace data transformation from end-to-end. Capturing changes across systems is easy with this form of data lineage since it tracks data as it moves. However, it requires a high level of understanding of the programming languages or tools used throughout the data lifecycle. To deploy this solution, one must understand all programming languages and tools used for transforming and moving the data, such as ETL tools, SQL queries, programming languages used to create pipelines. This can be a complex process but it provides comprehensive tracing capabilities.  

## Summary

In this post, we've seen more details about data processing and of what it consists. The data processing step starts with data cleansing (pre-processing) that makes data reliable and lasts with data lineage that keep track of the relationships between and within datasets and any data manipulation that was performed during data processing.

We've seen that various file formats exist to store and process data efficiently. Each format has its own advantages and disadvantages depending on the use cases it is intended for.

We've observed that Data transformation is the cornerstone of the data processing step. In this process data is converted and structured from a particular form into into a usable form that organization can utilize to gain insights and support decision making processes.

## References  

*   https://www.imperva.com/learn/data-security/data-lineage
*   https://docs.snowflake.com/fr/user-guide/data-sharing-intro.html
*   https://www.tibco.com/fr/reference-center/what-is-data-as-a-service-daas
*   https://www.gartner.com/en/documents/3957301/augmented-data-catalogs-now-an-enterprise-must-have-for-
*   https://www.talend.com/resources/what-is-data-preparation/
*   https://bi-survey.com/data-discovery
*   https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/
*   https://www.tableau.com/learn/articles/what-is-data-cleaning
*   https://www.oreilly.com/library/view/operationalizing-the-data/9781492049517/ch04.html
*   https://www.adaltas.com/en/2020/07/23/benchmark-study-of-different-file-format/
