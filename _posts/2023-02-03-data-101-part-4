---
layout: post
title: Data 101 series - part 4
subtitle: Choosing the Right Data Architecture for Your Needs
thumbnail-img: https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEivOCvukKciq8P8AIUcX63qSo_NGKAEzGTFB5kZR0x4v377O7GWzHVmObnnpz4k3Y_5qfsEj9T_92XWWNeDMPcok2iRH-b3z3-DeihydObcpL6nAcgzX7agbR85LUzqAtHJ2IQwKedZhbMhYFzdPn_Gxnq3_uuJTGx2W3w_YabT0nX248Rrp2_Cw8sy
share-img: https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEivOCvukKciq8P8AIUcX63qSo_NGKAEzGTFB5kZR0x4v377O7GWzHVmObnnpz4k3Y_5qfsEj9T_92XWWNeDMPcok2iRH-b3z3-DeihydObcpL6nAcgzX7agbR85LUzqAtHJ2IQwKedZhbMhYFzdPn_Gxnq3_uuJTGx2W3w_YabT0nX248Rrp2_Cw8sy
tags: [lambda,data fabric,data hub,stream processing,batch processing,kappa,data mesh]
comments: true
---

As I explained in the last posts, data architecture is a pivotal component for any data strategy. Unsurprisingly, choosing the right data architecture should be a top priority for many organizations. 

Data architectures can be classified based on data velocity. The two most popular velocity-based architectures are Lambda and Kappa. Data architectures can also be classified based on their operational mode or topology: Data Fabric, Data Hub, and Data Mesh are the three main architectures we'll discuss in this classification.

In this post, I'll dive deeply into the key features and characteristics of these architectures and provide a comparison to help you decide which one is the best fit for your business needs. Whether you’re a data scientist, engineer, or business owner, this guide will provide valuable insights into the pros and cons of each architecture and help you make an informed decision on which one to choose.

## Velocity-based data architectures

Data Velocity: refers to how quickly data is generated and how quickly that data moves and can be processed into usable insights. Depending on the velocity of data they process, data architectures can be classified into two categories: Lambda and Kappa.

In this chapter, I describe the two architectures in more details: their differences, the technologies we can use to realize them as well as the tipping points that will make us decide on using one or the other.

### 1 - Lambda data architecture

The term "Lambda" is derived from lambda calculus (**λ**) which describes a function that runs in distributed computing on multiple nodes in parallel. Lambda data architecture was designed to provide a scalable, fault-tolerant, and flexible system for processing large amounts of data and provides access to batch-processing and stream-processing methods in a hybrid way. It was developed in 2011 by Nathan Marz, the creator of Apache Storm, as a solution to the challenges of real-time data processing at scale.

The Lambda architecture is an ideal architecture, when you have a variety of workloads and velocities. It can handle large volumes of data and provide low-latency query results, making it suitable for real-time analytics applications like dashboards and reporting. This architecture is useful for batch processing (eg., cleansing, transforming, or aggregating data, for stream processing tasks (eg., event handling, machine learning models development, anomaly detection or fraud prevention), and building centralized repositories known as ‘data lakes’ to store structured/unstructured information. 

The critical feature of Lambda architecture is that it uses two separate processing systems to handle different types of data processing workloads. The first system is a batch processing system, which processes data in large batches and stores the results in a centralized data store (eg., a data warehouse or a data lake). The second system is a stream processing system, which processes data in real-time as it arrives and stores the results in a distributed data store (eg., in a stream topic/channel). 

Lambda architecture is used to solve the problem of computing arbitrary functions: evaluating the data processing function for any given input (in slow motion or in real-time). It provides fault tolerance by ensuring that the results from either system can be used as input into the other if one fails or becomes unavailable. The efficiency of this architecture becomes evident in the form of high throughput, low latency, and near-real-time applications.

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEheFA-MOIfmrZwz3jrwHXDhRI-4sQyXlzdU0zAxggrMIu0toS9JXiO8QWW3dAOFIiNB0sPgFuDMXsKzgaI_FbX--7YzMsGI17yVfZ5ZkEnqlAXLij6KQbVfTXy89IeP03PbVb9-F8GuU4_Z5HOEKmJ-vK7I5rpFmjLTUNqhueBlP16VWWqI43WWYgu5){: .mx-auto.d-block :} *Lambda architecture*{:style="display:block; margin-left:auto; margin-right:auto; text-align: center"} 
  
In the diagram above, you can see the main components of the Lambda Architecture. It consists of the ingestion layer, the batch layer, the speed layer (or stream layer), and the serving layer.   

*   **Batch Layer**: The batch processing layer is designed to handle large volumes of historical data and store the results in a centralized data store, such as a data warehouse or distributed file system. This layer utilizes frameworks like Hadoop or Spark for efficient processing of the information, allowing it to provide an overall view of all available data.
*   **Speed Layer**: The speed layer is designed to handle high\-volume data streams and provide up\-to\-date views of the information by using event processing engines, such as Apache Flink or Apache Storm. This layer processes incoming real\-time data and stores the results in a distributed data store such as a message queue or a NoSQL database.
*   **Serving Layer**: The serving layer of Lambda architecture is essential for providing users with a consistent and seamless access to data, regardless of the underlying processing system. Furthermore, it plays an important role in enabling real\-time applications like dashboards and analytics that need rapid access to current information.

While Lambda architectures offers a lot of advantages such as scalability, fault-tolerance and flexibility to handle a wide range of data processing workloads (batches and streams), it also comes with drawbacks that organizations need to consider before deciding on using it or not. In fact, Lambda architecture is a complex system that uses multiple technology stacks to process and store data. Every stage and the underlying logic is duplicated into the Batch Layer and the Speed Layer. It can be challenging to set up and maintain especially for organizations having limited resources. This duplication has a cost: data discrepancy as although having the same logic, the implementation is different from one layer to the other. Thus, the error/bug probability is definitively higher and you may run into a problem of different results from batch and speed layers. 

### 2 - Kappa data architecture

In 2014, when he was still working at LinkedIn, **_Jay Kreps_** started a discussion where he pointed out some drawbacks of the Lambda architecture. This discussion further led the big data community to another alternative that used less code resources.

The principale idea behind, is that a single technology stack can be used for both real-time and batch data processing. This architecture was called Kappa. Kappa architecture is named after the Greek letter "Kappa" (**ϰ**), which is used in mathematics to represent a "loop" or "cycle." The name reflects the architecture's emphasis on continuous processing of data or data reprocessing, rather than a batch-based approach. At its core, it relies on streaming architecture: incoming data is first stored in an event streaming log, then processed continuously by a stream processing engine, like Kafka, either in real-time or ingested into any other analytics database or business application using various communication paradigms such as real-time, near real-time, batch, micro-batch, and request-response.

Kappa architecture is designed to provide a scalable, fault-tolerant, and flexible system for processing large amounts of data in real time. The Kappa architecture is considered a simpler alternative to the Lambda architecture as it uses a single technology stack to handle both real-time and historical workloads, as it treats everything as streams. The main motivation for inventing the Kappa architecture was to avoid maintaining two separate code bases (pipelines) for the batch and speed layers. This allows it to provide a more streamlined and simplified data processing pipeline while still providing fast and reliable access to query results.

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhBz-J-hd9Ie3QPINUYBDkCbhW69xFXGH1paw3Ao5QQIe1oHNm8Eqr_tg_3q1efUfn5TFxqp0BNi4InUPVnddaeepTg9RVARQFxmYjk3zPomuiwNZ12wnGHGK9MSGT5J9Z1I4L0qY8208ZV7mrJ5riZ9megc8ghsRvEBH5KkJ-4jHlliOeU7z6ggM8J){: .mx-auto.d-block :} *Kappa architecture*{:style="display:block; margin-left:auto; margin-right:auto; text-align: center"}  

The most important requirement for Kappa, was Data reprocessing, making visible the effects of data changes on the results. As a consequence, the Kappa architecture is composed of only two layers: the stream layer and the serving one.

The stream layer runs the stream processing jobs to enable real-time data processing. Data is read and transformed immediately after it is inserted into the messaging engine, making recent (near) real-time analytics available for end users. The serving layer is used to query the results.

In Kappa architecture, there is only one processing layer: the stream processing layer. This layer is responsible for collecting, processing and storing live streaming data. This approach eliminates the need for batch-processing systems by utilizing an advanced stream processing engine such as Apache Flink, Apache Storm, Apache Kafka or Apache Kinesis to handle high volumes of data streams and provide fast, reliable access to query results. The stream processing layer is divided into two components: the ingestion component which collects data from various sources, and the processing component which processes this incoming data in real-time.

*   **Ingestion component**: This layer collects incoming data from a variety of sources, such as logs, database transactions, sensors, and APIs. The data is ingested in real-time and stored in a distributed data store, such as a message queue or a NoSQL database. 
*   **Processing component**: The Processing component of the Kappa architecture is responsible for handling high\-volume data streams and providing fast and reliable access to query results. It utilizes event processing engines, such as Apache Flink or Apache Storm, to process incoming data in real-time and historical data coming from a storage area before storing it in a distributed data store.

Nowadays, real-time data beats slow data. That’s true for almost every use case. Nevertheless, Kappa Architecture cannot be taken as a substitute of Lambda architecture. On the contrary it should be seen as an alternative to be used in those circumstances where active performance of batch layer is not necessary for meeting the standard quality of service. 

While Kappa architectures comes with the promises of scalability, fault-tolerance and streamlined management (simpler to set up and maintain compared to Lambda), it also has disadvantages that organizations need to consider carefully. Kappa architecture is theoretically simpler than Lambda, but it can still be technically complex for businesses that are not familiar with stream processing frameworks. However, the major drawback of Kappa, in my point of view, is the cost of infrastructure while scaling the event streaming platform. Storing high volumes of data in an event streaming platform can be costly and raises other scalability issues, especially when we are dealing with Terabytes or Petabytes. Moreover, the difference between event time and processing time, can lead to late-arriving data, which can represent a big challenge for a Kappa architecture: need for watermarking, state management, reprocessing, backfill...

## Topology-based data architectures

The debate between data architects has been focused on the merits of the different data ontologies that exist. While data architectures can be grouped in relation to data velocity as we've seen with Lambda and Kappa, another way to classify them is the data operational model or topology, which is technology-agnostic. Remember that three types of operating models can exist in any organization ([part 2](https://aelkouhen.github.io/2023-01-24-data-101-part-2/)): centralized, decentralized and hybrid.

In this chapter, I describe three topology-based data architectures in more details: data fabric, data mesh and data hub.

### 1 - Data Hub architecture

A data hub is an architecture for managing data in a centralized way. It can be described as a data exchange with frictionless data flow at its core. It acts as a central repository of information with connections to other systems and customers that allow the sharing of data between them. Endpoints interact with the Data Hub by providing data into it or receiving data from it, and the hub provides a mediation and management point, making visible how data flows across the enterprise. A data hub architecture facilitates this exchange by connecting producers and consumers of data together. The seminal work behind data hub architecture, was a Gartner research [paper](https://www.gartner.com/en/documents/3597517) published in 2017. In this paper, Gartner suggested a technology\-neutral architecture for connecting data producers and consumers, which was more advantageous than point\-to\-point alternatives. Subsequent research further developed this concept, resulting in the current definition of a data hub's attributes.

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiSGEDhwvOl03G83OuqVD2GZqCghZnC1LrsVkAyFZfQna_-sx4DNWcqRq4Wo0NVeytPYMxEPaCzYvf3VwSQuTlVpn6N8tgVfsis3386TfawoyHMZl4NzNwXT4hBZ5_ZJc5UUzw3JNA73RTg0JcgWkJtOCWtF4GxxKfvQ8icwKN8boxeuD38cN564s1Q/w472-h161/data%20hub%20attributes.png){: .mx-auto.d-block :} *Data hub attributes*{:style="display:block; margin-left:auto; margin-right:auto; text-align: center"}  

The hub is structured and consumed according to the models defined by its users. Governance policies are established to ensure data privacy, access control, security, retention and disposal of information in a secure manner. Integration strategies such as APIs or ETL processes can be used for working with the data stored within the hub. Persistence defines which type of database should be utilized for storing this data (e.g., relational databases). The implementation of a Data Hub architecture is a facilitator for:

*   Consolidating and streamlining the most commonly used data into a central point,
*   Establishing an effective data control system to improve data quality,
*   Ensuring traceability on flows and offering statistical monitoring of the company's activity,
*   Improving knowledge of the exchanged data,
*   Gradually building the company data model.

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCjvd3WDzzxMGviTm-M_GHFpd6OOhXnvat8P9r5m0z_-AGw0v3nuw75yI9IIV9MZ8-8wGZCg9vMZ9idtcUtacNrUft3ux7Nhc5JMpTuKVE_pWrBHpQnbILWWOz03DXhtCEqV5hViQFBA-fexRApMVwslBsPGHuy5NL0cX1UWcvowD81ez6Sm1APHxx/w379-h276/data%20hub.png){: .mx-auto.d-block :} *Data hub*{:style="display:block; margin-left:auto; margin-right:auto; text-align: center"}  

Gartner proposed that specialized, purpose\-built data hubs could be used for various purposes. These included analytics data hubs used for collecting and sharing information for downstream analytics processes; application data hubs used as domain context for specific applications or suites; integration data hubs designed to facilitate the sharing of data through different integration styles; master data hubs focused on distributing master data across enterprise operational systems and procedures; and finally, reference data hubs with similar goals but limited in scope to "reference" (e.g., commonly utilized codes).

Data centralization implemented in data hubs ensures that data is managed from a central source, but is designed to make the data accessible from many different points. It serves to minimize data silos, foster collaboration, and provide visibility into emerging trends and impacts across the enterprise. A centralized data view helps align data strategy with business strategy by delivering a 360° view of trends, insights, and predictions so that everyone in the organization can move in the same direction.

The challenge with data centralization is that without accelerators or some sort of self-service strategy in place, processes can be slow. Requests take longer and longer to get done. The business can’t move forward fast enough and opportunities for better customer experiences and improved revenue are lost simply because they can’t be achieved fast enough.

For these reasons, Gartner has recently updated the data hub concept to allow organizations running multiple hubs in an interconnected way. This way, data hub could keep advantage of data centralization and leverages decentralization by putting more responsibility power back out into the lines of business. 

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj4zVcTirEJM6bghNf3OQbw7OpV_SlBlx3JAcLQ0sp4mk2mQ1rPnoIKbY_wWAt8FmPNUmLJy-4ejHi_VokLpSPuuqACsGec8WdeL6658ek2DGuxLRKiYbDlvWQLh6Q5qNCR_Mo3y5Py7Hi7WYPgEmxEha1qZ3VnRbyJ8FfxjMabx8mgLX4xQjVidDZl){: .mx-auto.d-block :} *Specialized data hubs in a distributed mode*{:style="display:block; margin-left:auto; margin-right:auto; text-align: center"}  

### 2 - Data Fabric architecture

Data decentralization is a data management approach that eliminates the need for a central repository by distributing the storage, cleaning, optimization, output and consumption of data across different organizational departments. This helps to reduce complexity when dealing with large amounts of data as well as issues such as changing schema, downtime, upgrades and backward compatibility.

Data fabric was created first as a distributed data environment that enables the ingestion, transformation, management, storage and access of data from various repositories for use cases such as business intelligence (BI) tools or operational applications. It provides an interconnected web\-like layer to integrate different processes related to data by leveraging continuous analytics over current and inferred metadata assets. Additionally, it utilizes advanced techniques like active metadata management, semantic knowledge graphs and embedded machine learning/AutoML capabilities in order to maximize efficiency.

This concept was coined first in late 2016, by _Noel_ _Yuhanna_ of Forrester Research in his publication “_Forrester Wave: Big Data Fabric_.” In his paper, he described a technology\-oriented approach combines disparate data sources into one unified platform using Hadoop and Apache Spark for processing. The goal is to increase agility by creating an automated semantic layer that accelerates delivery of insights while minimizing complexity with streamlined pipelines for ingestion, integration and curation.

Over the years, Noel has developed his Big Data Fabric concept further. His current vision for this fabric is to provide solutions that meet a range of business needs such as creating an all\-encompassing view of customers, customer intelligence and analytics related to the Internet of Things. The data fabric includes components like AI/ML, data catalog, data transformation, data preparation, data modeling, and data discovery. It also provides governance and modeling capabilities which enable complete end-to-end management of data.

Gartner has also adopted the term "data fabric" and defined it in a similar way: they describe it as an emerging data management and integration design that enables flexible, reusable, and enhanced data integration pipelines, services, and semantics to support various operational or analytics use cases across multiple deployment platforms. Data fabrics combine different types of data integration techniques while utilizing active metadata, knowledge graphs, semantics, and machine learning (ML) to improve their design process. They defines five inner attributes as parts of data fabric:

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgW66S18yxSA1Ab3juAE2Fo22H4NRg7ecWfyoHazHHkRp4UB3CXFijN5DutUsLUOvkyXdMaV6IWvjn2TbxoaU6ZW7TyFt8hQq_BiiBm38W5pxeXJw-4AR0vMYdujEcBDKh0BDRY37tmA6Cmh-yFFLiHe9CT1O5CydZTfbnPCQ9CytL2Svub7XBxoqz8/w554-h163/data%20fabric.png){: .mx-auto.d-block :} *Data fabric attributes*{:style="display:block; margin-left:auto; margin-right:auto; text-align: center"}  
 
In a fabric, active metadata catalogs passive data elements such as schemas, field types and data values along with knowledge graph relationships. The knowledge graph, stores and visualizes the complex relationships between multiple data entities. It maintain data ontologies to help non-technical users to interpret data. 

The data fabric leverage AI and Machine Learning capabilities to automatically assist and enhance data management activities. This architecture should also offer integration capabilities to dynamically ingest disparate data into the fabric where it is stored, analyzed and accessed. Furthermore, automated data orchestration supports a variety of data and analytics use cases across the enterprise and allows users to apply DataOps principles throughout the process for agile, reliable and repeatable data pipelines. 

Data fabric is a technology-agnostic architecture. However, its implementation enables you to scale your big data operations for both batch processes and real-time streaming, providing consistent capabilities across cloud, hybrid multi-cloud, on premises, and edge devices. It simplifies the flow of information between different environments so that a complete set of up-to-date data is available for analytics applications or business processes. Additionally, it reduces time and cost by offering pre-configured components and connectors which eliminate the need to manually code each connection.

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgNtKTIX5ifJWMnuirR74BLVcz4XYm3sRqZeEshnDHge5bECx4zOzynkXIlZiAbLniW_qGS8C3F2GpNSgbMrxDcM8cWSOwfYO6DMOvfDR9xOQQ5c-eDq-tDXYDwLdoiScGyrdYAUK66Dm3qJBZaWpnN1IpY357wuJeg3B_VsTucpY58fC-3Imyd9ljY){: .mx-auto.d-block :} *Example of a data fabric ©Qlik*{:style="display:block; margin-left:auto; margin-right:auto; text-align: center"}   
  
### 3 - Data Mesh architecture

Data mesh was introduced in 2019 by **_Zhamak Dehghani_**. In her blog [post](https://martinfowler.com/articles/data-monolith-to-mesh.html), she argued that a decentralized architecture was necessary due to the shortcomings of centralized data warehouses and lakes.

A data mesh is a framework that enables business domains to own and operate their domain\-specific data without the need for a centralized intermediary. It draws from distributed computing principles, where software components are shared among multiple computers running together as a system. In this way, ownership of the data is spread across different business domains with each responsible for creating its own products. Additionally, it allows easier contextualization of the collected information to generate deeper insights while simultaneously facilitating collaboration between domain owners in order to create tailored solutions according to specific needs.

In a subsequent [article](https://martinfowler.com/articles/data-mesh-principles.html), **_Zhamak_** revised her position by proposing four principles that form this new paradigm.

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjeIGaztMPmFF0_INbeOPm48zODc_UW9LiZZqA1m9H4NLxaNuwf3tCLXpGvRdaue4IO122weY1JEriq8-D0QU1x0jiLeeh0X4UiGz-zNDlEzEQy0Ub0NvLyizaSX-eD573Y2rgRRzJa5dY8b8RqwY2F0EmokiTSKfXESWnmG6fjOfPCywdgV0lIDzOn/w487-h178/data%20mesh%20principles.webp){: .mx-auto.d-block :} *Data mesh principles*{:style="display:block; margin-left:auto; margin-right:auto; text-align: center"}   

* Domain-oriented: Data mesh is based on the principle of decentralizing and distributing responsibility for analytical data, its metadata, and the computation necessary to serve it to people who are closest to the data. This allows for continuous change and scalability in an organization's data ecosystem. To do this, Data Mesh decomposes components along organizational units or business domains which localizes any changes or evolution within that bounded context. By doing so, ownership of these components can be distributed across different stakeholders who are close to the data itself.

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIo7UicvbWQje4eL2xmZQRJxYCtR1F1YcEHlk2OBPeu317Ur4awj9kcZYZoaXI7ZwqKE15uxEkqQLA-onvB-Yir-hpUkj3OQjksQODaaAzrMaC50reJ03qg6yG3xqavfz2mAV1ZUKMm8zm7STak6e4HELz0Imr9p9qjQAkPqs6WefQlHYQJGbTCF3u){: .mx-auto.d-block :} *A data domain*{:style="display:block; margin-left:auto; margin-right:auto; text-align: center"}   

* Data-as-a-product: One of the issues with existing analytical data architectures is that it can be difficult and expensive to discover, understand, trust, and use quality data. If not addressed properly, this problem will only become worse as more teams provide data (domains) in a decentralized manner, which would violate the first principle. To address these challenges related to data quality and silos, a data mesh must treat analytical data provided by domains as a product and the consumers of that product as customers. The product will become the new unit of architecture that should be built, deployed, and maintained as a single quantum. It ensures that data consumers can easily discover, understand, and securely use high-quality data from across many domains.

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEivOCvukKciq8P8AIUcX63qSo_NGKAEzGTFB5kZR0x4v377O7GWzHVmObnnpz4k3Y_5qfsEj9T_92XWWNeDMPcok2iRH-b3z3-DeihydObcpL6nAcgzX7agbR85LUzqAtHJ2IQwKedZhbMhYFzdPn_Gxnq3_uuJTGx2W3w_YabT0nX248Rrp2_Cw8sy){: .mx-auto.d-block :} *A data product*{:style="display:block; margin-left:auto; margin-right:auto; text-align: center"}   

* Self-service data infrastructure: The infrastructure platform allows domain teams to autonomously create and consume data products without having to worry about the underlying complexity of building, executing, and maintaining secure and interoperable solutions. A self-service infrastructure should provide a streamlined experience that enables data domains owners to focus on their core objectives instead of worrying about technical details. The self-serve platform capabilities fall into multiple categories or planes:
  -   **_Infrastructure provisioning plane_** that supports the provisioning of the underlying infrastructure, required to run the components of a data product and the mesh of products.
  -   **_Product developer experience plane_** is the main interface that a data product developer uses. It simplifies and abstracts many of the complexities associated with supporting their workflow, providing an easier\-to\-use higher level of abstraction than what's available through the Provisioning Plane.
  -   **_Mesh supervision plane_** provides a way to manage and supervise the entire data product mesh, allowing for global control over all connected products. This includes features such as quality monitoring, performance, security protocols assessment, providing access control mechanisms, and more. By having these capabilities at the mesh level rather than individual product levels it allows for greater flexibility in managing large networks of interconnected data products.

* Federated governance: data mesh implementation requires a governance model that supports decentralization and domain self-sovereignty, interoperability through a dynamic topology and an automated execution of decisions by the platform. Integrating global governance and interoperability standards into the mesh ecosystem allows data consumers to gain value from combining and comparing different data products within the same system.

The data mesh combines these principles into a unified, decentralized and distributed system, where data product owners have access to common infrastructure that is self\-service enabled for the development of pipelines which share data in an open yet governed manner. This allows them to develop products quickly without sacrificing governance or control over their domain’s data assets.

![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiKtI94US9Q5kqDf0gM_bUYNz1rSidOkSgG6AppCy0xfP4VWAmknA48mZmcGoobpwj-hnG9v9TRhAGvwLz5IrDz0feM_0vmlkp7Os5PJSBDpzysNDxIHm98n5AO9qDqsjUUylb43wFXpg2uRx1LbdWNPnDcShoVjRO1JssiLd71OREWIzgkTGBgFQug){: .mx-auto.d-block :} *A data mesh*{:style="display:block; margin-left:auto; margin-right:auto; text-align: center"}   

The data mesh is quit different from the traditional approach that consists of managing the pipelines and data as separate entities, with shared storage infrastructure. Instead, it views all components (i.e, pipelines, data, and storage infrastructure) at the granularity of a bounded context within a given domain to create an integrated product. This allows for greater flexibility in terms of scalability and customization while also providing better visibility into how different parts interact with each other.

## Summary

The data market is often seen as being stagnant and mature, but the current discussion on data architecture proves this to be a wrong statement.

Data types are increasing in number, usage patterns have grown significantly, and there has been a renewed emphasis placed on building pipelines with Lambda and Kappa architectures in the form of data hubs or fabrics. Either grouped by velocity or but the kind of topology they provide, data architectures are not orthogonal: A data architecture is not a substitute of another one, it can be its complement. The data architectures and paradigms we presented in this post, can be used side-by-side, used alternately when there is need to alternate between them, and of course, they can be mixed in architectures like data mesh, in which each data product is a standalone artifact. We can imagine that a Lambda architecture is implemented in some data products and a Kappa architectures in many others. 

Despite all of these changes however, it's clear that the debate over how best to structure our approach to handling data is far from over - we're just getting started!

## References

*   https://nexocode.com/blog/posts/lambda-vs-kappa-architecture/
*   https://towardsdatascience.com/a-brief-introduction-to-two-data-processing-architectures-lambda-and-kappa-for-big-data-4f35c28005bb
*   https://hazelcast.com/glossary/lambda-architecture/
*   https://hazelcast.com/glossary/kappa-architecture/
*   https://www.kai-waehner.de/blog/2021/09/23/real-time-kappa-architecture-mainstream-replacing-batch-lambda/
*   https://www.ericsson.com/en/blog/2015/11/data-processing-architectures--lambda-and-kappa
*   https://www.stambia.com/en/solutions/by-project/data-hub-architecture  
*   https://www.qlik.com/blog/data-hub-fabric-or-mesh-part-1-of-2
*   https://www.qlik.com/us/data-management/data-fabric
*   https://www.qlik.com/blog/data-hub-fabric-or-mesh-part-2-of-2
*   https://martinfowler.com/articles/data-monolith-to-mesh.html
*   https://martinfowler.com/articles/data-mesh-principles.html
